{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "### v.1\n",
    "- initial build\n",
    "\n",
    "### v.2 \n",
    "- only use `isalpha()` words in dictionary/model\n",
    "- remove mispelled words by building difference of sets against `nltk.corpus.words.words()`\n",
    "\n",
    "### v.3\n",
    "- lots more notes added\n",
    "- more work on investigating spell correction routines \n",
    "\n",
    "### v.4 \n",
    "- re-ordered spellling routines\n",
    "- spell correcting is now online and part of the pipeline using `enchant` / `pyenchant`\n",
    "- I am making the assumption this code is pipelining a single school at a time\n",
    "- removing stopwords now after spelling correction\n",
    "\n",
    "### v.5\n",
    "- added variables for constants `MIN_WORD_COUNT`\n",
    "- now using `itertools.chain.from_iterable()` to flatten list of lists\n",
    "- now using a `collections.defaultdict()` (hash) to do word counts \n",
    "- word counts (due to the above two optimizations) are now near instantaneous\n",
    "- using `file_root` variable to propogate changing filenames throughout notebook\n",
    "- replaced nested comprehensions with `itertools`\n",
    "- added full parameters to LDA model\n",
    "- add logging so we can see any warnings from gensim\n",
    "- added random seed for debugging (not needed unless you want reproducable result)\n",
    "- added `dictionary.filter_extremes()` (still need to find best settings)\n",
    "\n",
    "### v.6\n",
    "- removed all LSI / LSA now just doing LDA\n",
    "- tuned LDA parameters for `passes=10` and `update_every=0` (batch mode) by default\n",
    "- added standard `matplotlib` setup from our student notebooks\n",
    "- added `remove_border()` from our student notebooks\n",
    "- added histograms for word frequency counts\n",
    "- added models for LDA from tradional bow as well as tf-idf\n",
    "- added comments at the top of most cells to give some indication of what runtime takes\n",
    "- added descriptive statistics on the dictionary\n",
    "\n",
    "### v.7\n",
    "- now using mysql database routines\n",
    "- now properly closing filehandle on file routines\n",
    "- create `read_review_file` function\n",
    "- create `clean_review_file` function\n",
    "- removed \"memory friendly\" routines which work with persistant disk\n",
    "\n",
    "### v.8\n",
    "- increased stopwords\n",
    "- add `WordNet` Lemmatization\n",
    "- removing all words `len(word) < 3` in `clean_reviews`\n",
    "- added bigram support (`CELL 110` controls whether unigrams or bigrams are used)\n",
    "- `CELL 130` and `CELL 230` will not work properly with bigrams in use so leave those commented out if using bigrams\n",
    "- `CELL 200` will not produce any output if using bigrams\n",
    "- labeled cells\n",
    "\n",
    "### v.9\n",
    "- `CELL 70` grab school names\n",
    "- `CELL 80` now pass a stop list to `clean_reviews`\n",
    "- `CELL 110` now always computing bigrams\n",
    "- added some additional stopwords to `custom_stopset`\n",
    "- `CELL 170` stitches unigram and bigram texts together for processing\n",
    "- `CELL 175` we chose which text to process: unigrams, bigrams or a unigram+bigram text\n",
    "- `CELL 210` now calling `corpus corpus_bow`\n",
    "- `CELL 240` now caling `tfidf corpus_tfidf`\n",
    "- added `CELL 265` choose a `corpus`\n",
    "- removed `CELL 280` (code factored into `CELL 270`)\n",
    "- I upgraded my own workstation to premium (free) versions of anaconda/accelerate, timings now reflect that\n",
    "- only now showing times for cells that take a while\n",
    "- moved `summary_statistics` into its own `CELL 25`\n",
    "- moved getting `clean reviews` from `CELL 90` to new `CELL 94`\n",
    "- moved descriptive statistics from `CELL 90` to new `CELL 96`\n",
    "- moved saving `corpus` from `CELL 210` to `CELL 212`\n",
    "- Added nicer headings \n",
    "- added information about installing distributed computing components\n",
    "- LDA now running in distributed mode\n",
    "\n",
    "### v.10\n",
    "- added `CELL 1500` HDP model for experimentation\n",
    "- added `MAX_NUM_REVIEWS` so you can reduce amount of reviews when testing\n",
    "- added `feature` dict to control what features are performed on text\n",
    "- added tokenize feature and step in `clean_reviews`, before this was being done when removing stopwords\n",
    "- re-worked entire `clean_reviews` function around `feature` dict\n",
    "- incorporated `feature` dict to control which cells are enabled\n",
    "- added pos tagging for later analysis\n",
    "- updated `CELL 1000` for exploratory analysis\n",
    "\n",
    "### v.11\n",
    "- added `CELL 95` to look at the text\n",
    "- corrected error in `CELL 60` where not intializing spell corrector unless `analyze_spell_correct` was set\n",
    "- now using `nltk.tokenize` for tokenization of sentences and words\n",
    "\n",
    "### v.12\n",
    "- added `CELL 8`5 to instantiate parallel processing\n",
    "- added `parallel` to `feature` dict\n",
    "- added decoration to `CELL 80` for parallel processing\n",
    "- modified `CELL 80` so it used `data_to_clean` and `stopset_to_use` as a name space hack\n",
    "- modified `CELL 90, 94` to use `data_to_clean` and `stopset_to_use`\n",
    "- modified `CELL 90` to use parallel processing for `school_names` if set\n",
    "- added `CELL 91` to merge parallel data back into single result for `school_names`\n",
    "- added parallel routines to `CELL 94`\n",
    "- `CELL 96` now renumbered to `CELL 98`\n",
    "- `CELL 95` now renumbered to `CELL 97`\n",
    "- added `CELL 96` to merge data returned from parallel workers for `texts`, `texts_uncorrected`, etc\n",
    "- move `bigram_stoplist` creation from `CELL 90` to `CELL 93`\n",
    "- `stopset_bigrams` is now just a set of bigrams of school names, nothing else\n",
    "- `CELL 97` now using `features` dict to control output\n",
    "- added `tfidf` to `features` dict to control corpus creation and dictionary transform\n",
    "- added cell magic `%%time` to `CELLS 90, 94, 96, 270`\n",
    "- added \"way\", \"thing\", and \"lot\" to `stopset_unigram`\n",
    "- factored code into `get_review_data()` for `CELL 97`\n",
    "- added `CELL 95` text diagnostics\n",
    "- tracked down bug in `CELL 80` `clean_reviews`, now imputing \"the\" instead of skipping `None` reviews in order to preserve indices across texts\n",
    "- `CELL 80` moved lowercase function after spell correct, this will allow pos tagging to work more effectively\n",
    "- `CELL 80`, factored out removal of smallwords (`len < 3`) into its own `remove_smallwords` step which can be set in `feature` dict\n",
    "- features in `features` dict are now listed in the order of processing\n",
    "- starts Notes section to notebook\n",
    "- added second pass of stopword removal after lemmatization in `CELL 80`\n",
    "- HyperThreading tested, tested with 8 virtual cores\n",
    "\n",
    "### v.13\n",
    "- added feature `only_tagged,` when set, only parts of speech in tag_set are used\n",
    "- added `tag_set` which contains part of speech tags we will use when feature `only_tagged` is set\n",
    "- removed `only_nn` as depricated by `only_tagged`\n",
    "- modified `CELL 90, 94` to distribute `tag_set` to remote workers\n",
    "- modified `CELL 80` to now use `tag_set`\n",
    "- added \"everybody\", \"everyone\", \"great\", \"excellent\" and \"part\" to stoplist\n",
    "- server setup on Amazon EC2, see Notes section for more info, entire notebook up and running fine\n",
    "- added notes about installing Anaconda's premium distro as a recommendation for faster BLAS libraries\n",
    "- `CELL 80` added conversion of POS tags from treebank/penn style to morphy style for passing into Lemmatizer (not being used yet)\n",
    "- added info about starting the Pyro nameserver to Installation\n",
    "- `clean_reviews()` is now renamed to `clean_data()` which is more reflective of what the function is doing\n",
    "\n",
    "### v.14\n",
    "- moved spellcheck routine to be above POS tagger\n",
    "- `CELL 80` map penn pos tags to morphy pos tags when pos_tag enabled for lemmatization\n",
    "- re-write `CELL 80` `clean_data` to be pos_tag aware on all routines, allowing lemmatization to use pos tags\n",
    "- added to `custom_stoplist` 'hill','valley','den','alto','crest','wood','land'\n",
    "- `CELL 1000` wrote `get_topic_data()`\n",
    "- added `perplexity_search` feature parameter\n",
    "- added perplexity grid search to `CELL 268`\n",
    "- added \"love\", \"amazing\", and \"okay\" to `custom_stoplist`\n",
    "- `CELL 97` is now `CELL 98`, `CELL 98` is now `CELL 99`\n",
    "- new `CELL 97` created to save variables `reviews, school_names, texts, texts_uncorrected, texts_unlemmatized, texts_pos, lemma_dict`\n",
    "- added `preprocess_save` to `feature` dict.  Saves pre-processed data in `CELL 97`\n",
    "- added `preprocess_load` to `feature` dict.  Loads pre-processed data in `CELL 70` instead of raw db/file data\n",
    "- skipping `CELLS 75, 80, 85, 90, 91, 94, 95 and 96` if `preprocess_load` set\n",
    "- `CELL 269` added to review results of perplexity search\n",
    "- added `preprocess_file` variable to use as filename for loading and saving preprocessed data\n",
    "\n",
    "### v.15\n",
    "- add import re to global imports and `CELL 85`\n",
    "- stripping escape characters in `CELL 70` using a `re` compile pattern if `remove_html` is set\n",
    "- added updated `mysql_ops` class in `CELL 30`\n",
    "- modified `CELL 70` to use updated `mysql_ops` syntax\n",
    "- identified bug in `gensim`, filed on [github](https://github.com/piskvorky/gensim/issues/144#issuecomment-29562894), proposed a solution, fixed\n",
    "\n",
    "\n",
    "### v.16\n",
    "- updated formatting and flow to `CELL 268`\n",
    "- removed unigram and bigram frequency count historgrams from `CELL 150, 151`\n",
    "- merged `CELL 160` into ` CELL 150 and `CELL 161` into `CELL 151`\n",
    "- multiple additional comments added to cells\n",
    "- `CELL 1500` (HDP model) removed\n",
    "- `CELL 140` removed, as its functionality is already handled by `dictionary.filter_extremes()` in `CELL 180`\n",
    "- Now saving and loading `nces_code` and `universal_id` as `reviews_indexes` in `CELL 70` and `CELL 97`\n",
    "- Added routine to extract beta, gamma and log probabilities and save them to a file in `CELL 95`\n",
    "\n",
    "### v.17\n",
    "- code from `CELL 270` moved to new `CELL 292` to build `corpus_model`\n",
    "- modified `CELL 270` to store `num_topics` as a global variable\n",
    "- modified `CELL 290` to save model using `file_root`\n",
    "- added `CELL 295` to save `corpus_model`\n",
    "- added `CELL 310` to store `gamma` and `beta` using `file_root`\n",
    "- `CELL 97` now stores files individually using `file_root` as a base\n",
    "- modify `CELL 80` to work with situation where `pos_tag` is set but `only_tagged` is not set\n",
    "- added `dict_corpus_save` feature to control saving of dictionary and corpus in `CELL 182` and `CELL 212`\n",
    "- added `dict_corpus_load` feature to control loading of dictionary and corpus in `CELL 205`\n",
    "- added `model_save` feature to control saving of model in `CELL 290`\n",
    "- added `model_load` feature to control loading of model in `CELL 289`\n",
    "- added global variable `num_topics` in `CELL 0` which is used for creation of filenames to be loaded/saved and LDA model\n",
    "- added feature `beta_gamma_save` to save `beta` and `gamma` of model in `CELL 310`\n",
    "- added feature `model_topics_save` to save `model_topics` in `CELL 302`\n",
    "- added feature `corpus_model_save` to save `corpus_model` in `CELL 296`\n",
    "- added feature `model_topics_load` to load `model_topics` in `CELL 300`\n",
    "- added `CELL 300` to view model_topics\n",
    "- added feature `corpus_model_load` to load `corpus_model` in `CELL 293`\n",
    "- moved code from `CELL 292` to new `CELL 295` for viewing `corpus_model`\n",
    "- added feature `texts_final_load` to load `texts_final` in `CELL 160`\n",
    "- added feature `texts_final_save` to save `texts_final` in `CELL 178`\n",
    "\n",
    "### v.18\n",
    "- updated `CELL 70` to show filenames and sizes as they are loaded\n",
    "- updated `CELL 97` to show filenames and sizes as they are loaded\n",
    "\n",
    "### v.19\n",
    "- `CELL 40` which dealt with reading from a file has been removed\n",
    "- No longer using `preprocess_file` to save all texts to, instead saving each one separately\n",
    "- Removed `CELL 120` and `CELL 130` which dealt with analyzing our spell correction\n",
    "- Factored `CELL 150` and `CELL 151` into a function `frequent_tokens` in `CELL 150`\n",
    "- `CELL 151` now displays frequency counts of unigrams, `CELL 152` displays frequency counts of bigrams\n",
    "- `feature preprocess_load` and `texts_final_load` are mutually exclusive otherwise you may get errors\n",
    "- added `CELL 1100` to look at how to start getting data ready to be handed off to the database\n",
    "- numerous logic corrections in how feature's are handled and what CELLs should run\n",
    "- added `id` to `reviews_indexes` in `CELL 70`\n",
    "\n",
    "### v.20\n",
    "- added `postdate` to `reviews_indexes`\n",
    "- `CELL 1500` through `CELL 1580` added to output Word Frequency Counts on various slices and cuts of the data \n",
    "- `CELL 150` now using NLTK's FreqDist() function\n",
    "\n",
    "### v.21\n",
    "- `CELL 10` and `CELL 20` removed\n",
    "- copious amounts of annotation added for final submission\n",
    "\n",
    "\n",
    "### TODO/CONCERNS\n",
    "- factor out pickle read/write functions\n",
    "- `CELL 70` convert db functions to variables\n",
    "- Many more notes to write\n",
    "- Is the stopset ideal?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "It is recommended you install the premium version of Anaconda, Accelerate and IOPro from [Continuum](https://store.continuum.io/cshop/anaconda/).  \n",
    "\n",
    "You will see that there is an \"All Products are free for Academic Use\" link in the upper right, where you can obtain\n",
    "free licensing.  This will give you, among many other benefits, faster BLAS libraries.  You can verify your BLAS libraries here:\n",
    "\n",
    "    import numpy as np\n",
    "    np.__config__.show()\n",
    "\n",
    "You should install `gensim`\n",
    "\n",
    "    pip install gensim\n",
    "You should install `nltk`\n",
    "    \n",
    "    pip install nltk\n",
    "You will need to install specific modules from `nltk`.  They are `stopwords, punkt, wordnet, maxent_treebank_pos_tagger`. Fire up a `python` interpretor and run\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "You should install `pymysql`\n",
    "\n",
    "    pip install pymysql\n",
    "\n",
    "If you get any errors about `error: invalid command 'egg_info'` you just need to install `setuptools`, as our original \n",
    "anaconda distribution in class used a package called `distribute` but parts of that have been factored out to `setuptools`\n",
    "\n",
    "    pip install --upgrade setuptools\n",
    "\n",
    "You should install `Enchant` for spell check capability, http://www.abisource.com/projects/enchant/ \n",
    "I used [homebrew](http://brew.sh) to install it on OSX, `brew install enchant`\n",
    "You should then install `pyenchant`\n",
    "    pip install pyenchant\n",
    "\n",
    "####iPython Parallel Processing\n",
    "iPythons parallel processing is handled through an architecture called [ipcluster](http://ipython.org/ipython-doc/dev/parallel/parallel_process.html).  We use this in our notebook to parallelize the text pre-processing pipeline.\n",
    "\n",
    "first start an ipython `ipcluster` on your machine.  For example, for 4 cores:\n",
    "\n",
    "    ipcluster start -n 4 \n",
    "\n",
    "- most chips, such as Intel, support HyperThreading, so you can usually run say 8 virtual cores on a 4 physical core chip\n",
    "- make sure `parallel` feature is set in `feature` dict\n",
    "- make sure the decoration in `CELL 80` is uncommented `@dv.remote(block=True)`\n",
    " \n",
    "you could also start the `ipcluster` right from within the iPython notebook using the \"Cluster\" tab\n",
    " \n",
    "#### Gensim Distributed Computing for LDA\n",
    "[Gensim](http://radimrehurek.com/gensim/) leverages a distributed RMI type architecture called [Pyro4](http://pythonhosted.org/Pyro4/)\n",
    "To distribute the workload to other cores or computers on the same broadcast domain\n",
    "install `gensim[distributed]`\n",
    "    \n",
    "    pip install gensim[distributed]  (or \"pip install --update gensim[distributed]\" if you already installed previously)\n",
    "this installs Pyro4 as well.  Pyro4 is the distributed framework that `gensim` utilizes.\n",
    "\n",
    "set environment variables (these must be set in the session you use to start the notebook from)\n",
    "    \n",
    "    export PYRO_SERIALIZERS_ACCEPTED=pickle\n",
    "    export PYRO_SERIALIZER=pickle\n",
    "\n",
    "start the `Pyro4` nameserver\n",
    "\n",
    "    python -m Pyro4.naming -n 0.0.0.0 &\n",
    "start workers (ex: on a four core system to start four workers)\n",
    "    \n",
    "    python -m gensim.models.lda_worker &\n",
    "    python -m gensim.models.lda_worker &\n",
    "    python -m gensim.models.lda_worker &\n",
    "    python -m gensim.models.lda_worker &\n",
    "start dispatcher (just runs on one node, can also be a worker node)\n",
    "\n",
    "    python -m gensim.models.lda_dispatcher &\n",
    "make sure in the model, you have `distributed=True` option set\n",
    "that's it, you should see info about the distributed computing in the logs as you run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "#### CELL 0\n",
    "`CELL 0` contains a number of options which control the operation of the notebook.\n",
    "\n",
    "The notebook can be navigated in an ad-hoc way, skipping cells you do not wish to use.  To make various routine tasks easier, as well as provide control over things like the text pipeline, a dictionary called `feature` is used.  This is self-documented in `CELL 0`.  An important thing to note is that many of the features could be enabled simultaneously but may not make sense or may give undesirable output.  There is little to no dependency checking. \n",
    "\n",
    "##### Used by the Database\n",
    "`MAX_NUM_REVIEWS` can be set.  This will control how many reviews are ingested if using the database.  If set to `0` then all reviews returned by the function being used will be included.\n",
    "\n",
    "`state` can be set, and if retrieving from the database it will retrieve that states data.  If this is set to `all` or `ALL` it will retrieve data for all states.  \n",
    "\n",
    "\n",
    "##### Used when loading or saving to files\n",
    "`data_dir` can be set as the location to use when loading/saving texts, corpus, dictionaries, models, etc.\n",
    "\n",
    "`file_root` can be set, this controls the naming of all the many files that are loaded and saved.  You need to be careful what this is set to if any of the \"save\" bits are set in the `feature` dict, as you could overwrite previously saved data if not careful.\n",
    "\n",
    "##### Used by the text processing pipeline\n",
    "`tag_set` can be set to control which parts of speech are included when `only_tagged` is used in conjunction with feature `pos_tag`.  The various parts of speech that can be set here are explained at [Penn Treebank Tags](http://bulba.sdsu.edu/jeanette/thesis/PennTags.html).\n",
    "\n",
    "`custom_stopset` is a list of words which is removed from consideration in any model processing.  This is unioned with the common set of english stopwords found in NLTK's `nltk.corpus.stopwords.words('english')` to produce `stopset_unigram`.  This is the stoplist we use to remove words for any consideration in unigram processing.\n",
    "\n",
    "`stopset_unigram` is a unioned set of the above mentioned `custom_stopset` and NTLK's `nltk.corpus.stopwords.words('english')`.\n",
    "\n",
    "##### Used by LDA\n",
    "`num_topics` sets the number of topics to be used when buiding the LDA model.\n",
    "\n",
    "#### CELL 175\n",
    "\n",
    "In `CELL 175` you choose which text will go on to be considered in LDA.  Normally this is `text_combined` which is a combination of `texts` and `texts_bigrams` produced in `CELL 170`\n",
    "\n",
    "#### CELL 180\n",
    "\n",
    "In `CELL 180` you choose what tokens make it into the dictionary. By default we have this set to drop any tokens that appear in less than 5 documents.  We also drop any token that appears in more than 50% of all documents.  We also keep only the top 100000 most frequent tokens.  These can be set as desired.\n",
    "\n",
    "#### CELL 240\n",
    "\n",
    "`CELL 240` will transform the corpus via `tfidf` if set in `feature` dict.  LDA requires that actual values be used in the bag of words.  Converting to real numbers, as with `tfidf` may be beneficial, but it is not a valid transormation for traditional LDA as published.  So this feature has been left in place since earlier experiments in this project began, but all of our work for the most part has been exclusively using Bag of Words, without any transformation, including TF-IDF.\n",
    "\n",
    "#### CELL 268\n",
    "\n",
    "`CELL 268` can be used to do a grid search over any variable you wish.  The current stable branch of `gensim` however has a bug, which has been fixed in the [pyro_threads](https://github.com/piskvorky/gensim/tree/pyro_threads) branch.  So it is recommended that you install this branch if you wish to use this feature.  Otherwise, the Pyro4 underlying framework will run out of threads.  If not doing a grid search then the current release version of gensim[distributed] will work fine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "`CELL` numbers are used at the top of each cell just as a label so they can be tracked in changes as they are happening and discussed.\n",
    "\n",
    "#### Texts\n",
    "There are currently the following key texts created in the pipeline depending on what options are enabled.  These texts are all for diagnostic purposes, the only one actually being used by LDA is the final output `texts`.  The other texts are being created because of their place in the pipeline, allow us to see how certain transformations are performing, such as analyzing spell correction.  The order of creation is below:\n",
    "\n",
    "1. `reviews_indexes`\n",
    "2. `reviews`\n",
    "3. `school_names`\n",
    "4. `texts_uncorrected` \n",
    "5. `texts_pos`\n",
    "6. `texts_unlemmatized`\n",
    "7. `lemma_dict`\n",
    "8. `texts`\n",
    "\n",
    "A summary of each text:\n",
    "\n",
    "- `reviews_indexes` - this is a copy of the original gsid, nces_code, universal_id and postdate in a dataframe\n",
    "- `reviews` - this is the data as ingested from the file or the database\n",
    "- `school_names` - this is the school names pulled from the db AFTER they have been through pre-processing.  Since most words in school names are not in our tag set (NN or NNS for example), and many words in school names are stopped, this list is quite sparse.  It is used only for the  creation of a bigram_stopset, so that any bigrams created involving school names are not used.\n",
    "- `texts_uncorrected` - this is the text before spell correction\n",
    "- `texts_pos` - this is the text after it has been tagged with parts of speech\n",
    "- `texts_unlemmatized` - this is the text before lemmatization\n",
    "- `lemma_dict` - This is a dictionary of a set of words.  It is used just as an analysis to see what words were converted to what lemmas.  \n",
    "- `texts` - this is the final text which is used further on in the notebook and is the comprises the bulk of what is inputed into LDA\n",
    "\n",
    "There are various filters and transformations being done on the text.  Here is a workflow of where the transforms and filters are done, in relation to where the texts are being created.  All texts are created in `CELL 80 clean_data()` function.\n",
    "\n",
    "#### General Data Cleaning Pipeline:\n",
    "\n",
    "    reviews and school_names created in CELL 70\n",
    "        clean_data() called in CELL 80\n",
    "            impute \"the\" if review == None\n",
    "            remove HTML encodings and escape characters\n",
    "            tokenize documents into sentences\n",
    "            tokenize sentences into words                      * texts_uncorrected created\n",
    "            spell correct words\n",
    "            POS tag words                                      * texts_pos created\n",
    "            remove punctuation\n",
    "            remove small words (len < 3)\n",
    "            remove non-alpha words\n",
    "            lowercase all words\n",
    "            remove stopwords                                   * texts_unlemmatized created\n",
    "            lemmatize all words\n",
    "            lemma_dict created\n",
    "            remove small words (len < 3) (again)\n",
    "            remove stopwords (again)                           * texts created\n",
    "            \n",
    "\n",
    "A server has been configured on [Amazon EC2](http://aws.amazon.com/ec2/instance-types/) for our final analysis.\n",
    "\n",
    "This server is a Compute Optimized `c3.8xlarge` instance.  It has the following specifications:\n",
    "CPU: 64-bit vCPU: 32 eCPU: 108 Memory: 60 GB  Processor: Intel Xeon E5-2680\n",
    "\n",
    "# Text Processing Pipeline\n",
    "## Text Processing\n",
    "Text processing was done using Pythons’s [Natural Language Toolkit (NLTK)](http://nltk.org) and [iPython clustering](http://ipython.org/ipython-doc/dev/parallel/).  [GreatSchools](http://www.greatschools.org) reviews were ingested from the database, scattered to multiple ipython worker nodes, processed and finally merged back together into one text.\n",
    "\n",
    "Text processing followed the following workflow:\n",
    "\n",
    "![Alt text](files/img_bf-notebook-0.png)\n",
    "\n",
    "The premium version of [Continuum’s Anaconda](https://store.continuum.io/cshop/anaconda/), with [Accelerate](https://store.continuum.io/cshop/accelerate/), [IOPro](https://store.continuum.io/cshop/iopro/) and [MKL Optimizations](https://store.continuum.io/cshop/mkl-optimizations/) was used.  This distribution provided faster [BLAS](http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) libraries and greatly optimized NumPy operations.\n",
    "\n",
    "The data from GreatSchools was in a MySQL database and that was the method used for ingesting the data into the text pipeline.\n",
    "\n",
    "## Sharding the reviews data\n",
    "\n",
    "iPython’s parallel clustering capabilities were leveraged.  This allows each core to represent a worker node which can work on part of the overall task.  With a 32 core system you can have 32 worker nodes, theoretically allowing you to process data in 1/32 the time of using a single core.  iPython has a scatter function which will take a dataset and shard it to all the workers available.  This was used to take the complete set of GreatSchools data and shard it across all workers.  In our production system we used an [Amazon c3.8xlarge](http://aws.amazon.com/ec2/instance-types/#instance-details), which is a 32 core system with 60GB of memory.  We supplemented the physical memory with 20GB of “virtual memory” using a swapfile, thus giving each core approximately 28,000 reviews to work on with 2.5GB of memory each.\n",
    "\n",
    "![Alt text](files/img_bf-notebook-2.png)\n",
    "\n",
    "## Order of text processing pipeline\n",
    "\n",
    "During the course of working on the project, the order of operations being performed in the text pipeline changed.  There are many ways in which the operations can be combined, and most orderings involve a trade-off between performance and extracting the cleanest text.  For example, it is important that punctuation remain in place for part of speech (pos) tagging to perform optimally.  It is also important for words to be spelled correctly for a pos tagger to work correctly.  Similarly, it is difficult for a pos tagger to identify all Proper Nouns if you lowercase all text beforehand.  The final order of operations was decided on as being the most optimal for providing meaningful and clean nouns, which would be used as inputs to a topic model built using LDA.  \n",
    "\n",
    "![Alt text](files/img_bf-notebook-3.png)\n",
    "\n",
    "The text contains various [escape character](http://en.wikipedia.org/wiki/Escape_character) sequences and HTML encodings that needed to be removed, as not all the functions being used in text processing could handle them.  Basic string `replace()` methods and `re sub()` methods were used to handle this.\n",
    "\n",
    "The text was next tokenized using the [NLTK sentence and word tokenizers](http://nltk.org/api/nltk.tokenize.html).\n",
    "\n",
    "After tokenization, the text was spell corrected using the [Enchant spell correction library](http://www.abisource.com/projects/enchant/) from [Abiword](http://www.abisource.com) and its python helper library [pyenchant](http://pyenchant.readthedocs.org/en/latest/).  There were numerous spelling errors.  This would result in fragmentation of the analysis (word counts, parts of speech tagging, topic creation, etc.), and so it was decided to spell correct.  This was one of the most expensive operations in the text processing,.  Each word was compared to an English dictionary, and if no match was found, the most likely candidate was selected as long as it was within an edit distance of 2.\n",
    "\n",
    "Part of speech tagging was performed using [NLTK’s Treebank maximum entropy classifier](http://nltk.org/api/nltk.classify.html) based pos tagger.  This is a slow but accurate pos tagger that was originally trained on the [Treebank](http://en.wikipedia.org/wiki/Treebank) corpus.  Ideally, we would have used a classifier that was trained on GreatSchools reviews or reviews data in general, but we felt the accuracy was good enough for our work.  We were looking at using only nouns and plural nouns in our topic model, and the default classification for almost all pos taggers including this one, is to use classify as a noun.  Most topic models are built using nouns as they are most representative of concrete topics.  Although adjectives, verbs and other parts of speech can add interesting context, they generally overpower the underlying nouns and can inject sentiment.  We were not using the words to classify sentiment, so only nouns were of interest.  Only nouns were further passed on in the pipeline.\n",
    "\n",
    "Once pos tagging was performed we then removed punctuation and words that were less than 3 characters.  These words would add no value to our topic model, and removing them greatly reduces the amount of data we are working with.\n",
    "\n",
    "All words were then lowercased using the normal python string `lower` method.\n",
    "\n",
    "Next, stop words were removed.  The stop words started with a list of common English stop words from the [NLTK stopword corpus](http://nltk.googlecode.com/svn/trunk/doc/api/nltk.corpus-module.html).  Additional words were manually added to create a `custom_stopset`, which we used to stop against all unigrams.\n",
    "\n",
    "Next words were lemmatized using [NLTK’s WordNet Lemmatizer](http://nltk.org/_modules/nltk/stem/wordnet.html).  This was to reduce the dimension of data even further by aggregating words that either are the same root or have the same meaning.  The lemmatizer uses the pos tags to understand what words might be synonyms.  The lemmatizer would take a word such as *teacher* and *teachers* and make sure both were just *teacher*.  This kept us from fragmenting topics.  This operation was expensive but we felt it improved the quality of data considerably.  Other techniques were looked at such as stemming, but lemmatization appeared to be superior for our purpose, as we wanted to make sure the words that were used were actual words humans would understand, and stemmers can aggressively mangle words. \n",
    "\n",
    "## Example of a review moving through the pipeline\n",
    "\n",
    "`State California\n",
    "Data from reviews for review number 4\n",
    "GSID: 214510 NCES Code: 060177000041    Universal ID: 600001`\n",
    "\n",
    "#### Original Review Text\n",
    "\n",
    "`We just went to Open House at Alameda High, and found the teachers very dedicated.  They know their students and were ready to give individual feedback to parents.  There is a range of academic levels offered including some very challenging AP classes. Some classes are rather large, but the EXP and AP classes may be smaller. There is a range of students, from those who are not interested in studying, to those who are highly motivated and hard working, with GPA's over 4.0. There is a lot of student involvement in extracurricular activities, such as a poetry anthology they were selling to raise money for the English Dept., a book sale to benefit the media center, a blood drive sponsored by one of the clubs.  They have after school sports, an active music dept (the jazz band just performed at Yoshi's), extraodinary drama dept. that puts on several plays each year.`\n",
    "\n",
    "#### Tokenized Text with encodings removed\n",
    "\n",
    "`['We', 'just', 'went', 'to', 'Open', 'House', 'at', 'Alameda', 'High', ',', 'and', 'found', 'the', 'teachers', 'very', 'dedicated', '.', 'They', 'know', 'their', 'students', 'and', 'were', 'ready', 'to', 'give', 'individual', 'feedback', 'to', 'parents', '.', 'There', 'is', 'a', 'range', 'of', 'academic', 'levels', 'offered', 'including', 'some', 'very', 'challenging', 'AP', 'classes', '.', 'Some', 'classes', 'are', 'rather', 'large', ',', 'but', 'the', 'EXP', 'and', 'AP', 'classes', 'may', 'be', 'smaller', '.', 'There', 'is', 'a', 'range', 'of', 'students', ',', 'from', 'those', 'who', 'are', 'not', 'interested', 'in', 'studying', ',', 'to', 'those', 'who', 'are', 'highly', 'motivated', 'and', 'hard', 'working', ',', 'with', 'GPA\\\\', \"'s\", 'over', '4.0', '.', 'There', 'is', 'a', 'lot', 'of', 'student', 'involvement', 'in', 'extracurricular', 'activities', ',', 'such', 'as', 'a', 'poetry', 'anthology', 'they', 'were', 'selling', 'to', 'raise', 'money', 'for', 'the', 'English', 'Dept.', ',', 'a', 'book', 'sale', 'to', 'benefit', 'the', 'media', 'center', ',', 'a', 'blood', 'drive', 'sponsored', 'by', 'one', 'of', 'the', 'clubs', '.', 'They', 'have', 'after', 'school', 'sports', ',', 'an', 'active', 'music', 'dept', '(', 'the', 'jazz', 'band', 'just', 'performed', 'at', 'Yoshi\\\\', \"'s\", ')', ',', 'extraodinary', 'drama', 'dept', '.', 'that', 'puts', 'on', 'several', 'plays', 'each', 'year', '.']`\n",
    "\n",
    "#### Text after spell correction and part of speech tagging\n",
    "\n",
    "`[('We', 'PRP'), ('just', 'RB'), ('went', 'VBD'), ('to', 'TO'), ('Open', 'NNP'), ('House', 'NNP'), ('at', 'IN'), ('Alarmed', 'NNP'), ('High', 'NNP'), (',', ','), ('and', 'CC'), ('found', 'VBN'), ('the', 'DT'), ('teachers', 'NNS'), ('very', 'RB'), ('dedicated', 'VBN'), ('.', '.'), ('They', 'PRP'), ('know', 'VBP'), ('their', 'PRP$'), ('students', 'NNS'), ('and', 'CC'), ('were', 'VBD'), ('ready', 'RB'), ('to', 'TO'), ('give', 'VB'), ('individual', 'JJ'), ('feedback', 'NN'), ('to', 'TO'), ('parents', 'NNS'), ('.', '.'), ('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('range', 'NN'), ('of', 'IN'), ('academic', 'JJ'), ('levels', 'NNS'), ('offered', 'VBD'), ('including', 'VBG'), ('some', 'DT'), ('very', 'RB'), ('challenging', 'VBG'), ('AP', 'NNP'), ('classes', 'NNS'), ('.', '.'), ('Some', 'DT'), ('classes', 'NNS'), ('are', 'VBP'), ('rather', 'RB'), ('large', 'JJ'), (',', ','), ('but', 'CC'), ('the', 'DT'), ('EXP', 'NNP'), ('and', 'CC'), ('AP', 'NNP'), ('classes', 'NNS'), ('may', 'MD'), ('be', 'VB'), ('smaller', 'JJR'), ('.', '.'), ('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('range', 'NN'), ('of', 'IN'), ('students', 'NNS'), (',', ','), ('from', 'IN'), ('those', 'DT'), ('who', 'WP'), ('are', 'VBP'), ('not', 'RB'), ('interested', 'JJ'), ('in', 'IN'), ('studying', 'NN'), (',', ','), ('to', 'TO'), ('those', 'DT'), ('who', 'WP'), ('are', 'VBP'), ('highly', 'RB'), ('motivated', 'VBN'), ('and', 'CC'), ('hard', 'RB'), ('working', 'VBG'), (',', ','), ('with', 'IN'), ('GPA', 'NNP'), ('S', 'NNP'), ('over', 'IN'), ('4.0', 'CD'), ('.', '.'), ('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('student', 'NN'), ('involvement', 'NN'), ('in', 'IN'), ('extracurricular', 'JJ'), ('activities', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('a', 'DT'), ('poetry', 'NN'), ('anthology', 'NN'), ('they', 'PRP'), ('were', 'VBD'), ('selling', 'VBG'), ('to', 'TO'), ('raise', 'VB'), ('money', 'NN'), ('for', 'IN'), ('the', 'DT'), ('English', 'NNP'), ('Dept', 'NNP'), (',', ','), ('a', 'DT'), ('book', 'NN'), ('sale', 'NN'), ('to', 'TO'), ('benefit', 'VB'), ('the', 'DT'), ('media', 'NNS'), ('center', 'NN'), (',', ','), ('a', 'DT'), ('blood', 'NN'), ('drive', 'NN'), ('sponsored', 'VBD'), ('by', 'IN'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('clubs', 'NNS'), ('.', '.'), ('They', 'PRP'), ('have', 'VBP'), ('after', 'IN'), ('school', 'NN'), ('sports', 'NNS'), (',', ','), ('an', 'DT'), ('active', 'JJ'), ('music', 'NN'), ('dept', 'NN'), ('(', ':'), ('the', 'DT'), ('jazz', 'NN'), ('band', 'NN'), ('just', 'RB'), ('performed', 'VBN'), ('at', 'IN'), ('Yoshi\\\\', 'NNP'), ('S', 'NNP'), (')', 'NNP'), (',', ','), ('extraordinary', 'JJ'), ('drama', 'NN'), ('dept', 'NN'), ('.', '.'), ('that', 'IN'), ('puts', 'NNS'), ('on', 'IN'), ('several', 'JJ'), ('plays', 'NNS'), ('each', 'DT'), ('year', 'NN'), ('.', '.')]`\n",
    "\n",
    "#### Text after lowercase, removal of stopwords, removal of small words, removal of punctuation\n",
    "\n",
    "`['feedback', 'range', 'levels', 'classes', 'classes', 'classes', 'range', 'studying', 'involvement', 'activities', 'poetry', 'anthology', 'money', 'book', 'sale', 'media', 'center', 'blood', 'drive', 'clubs', 'sports', 'music', 'dept', 'jazz', 'band', 'drama', 'dept', 'puts', 'plays']`\n",
    "\n",
    "#### Text after lemmatization\n",
    "\n",
    "`['feedback', 'range', 'level', 'class', 'class', 'class', 'range', 'studying', 'involvement', 'activity', 'poetry', 'anthology', 'money', 'book', 'sale', 'medium', 'center', 'blood', 'drive', 'club', 'sport', 'music', 'dept', 'jazz', 'band', 'drama', 'dept', 'put', 'play']`\n",
    "\n",
    "The text you see above only contains nouns (pos types NN and NNS).  This is what was the final output for this particular review, which we would use for later processing (imputation of bigrams, and then fed into our LDA model).\n",
    "\n",
    "## Merging the reviews data\n",
    "\n",
    "After the data was independently processed by each worker, it was then merged back together into a single text.  This text had the same number of documents as the original text.  The difference was that the text was processed, and basically reduced down to nouns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 0\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import operator\n",
    "import pymysql\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# below for saving/loading files\n",
    "import pickle\n",
    "# below for search/replace of escape characters\n",
    "import re\n",
    "# below two imports needed for spell correction\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "# below for bigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import bigrams\n",
    "# below for part of speech tagging\n",
    "# uses 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
    "from nltk.corpus import wordnet\n",
    "# below for tokenizing\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# below for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# set seed (only for debugging, no reason to do this unless you want repeatable result)\n",
    "# One area I did find it useful, was in experimenting with grid searches\n",
    "# np.random.seed(42)\n",
    "# random.seed(42)\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# features, listed in the order of processing\n",
    "feature = {\n",
    "\n",
    "           # Text Pre-Processing\n",
    "           # The features below concern themselves with the text pre-processing pipeline.  For example the only time we use ipythons \n",
    "           # ipcluster is during text pre-processing.  Be very careful if you have any \"save\" bits, set.  You will want to make sure \n",
    "           # that your file_root (defined below the feature dict) is set to a correct value, so that you do not overwrite data you\n",
    "           # do not wish to.\n",
    "           \n",
    "           # Note: preprocess_load and texts_final_load are mutually exclusive.  It would not make sense to set them both, there\n",
    "           # are other combinations that don't make sense that could lead to errors as well.\n",
    "           \n",
    "           'parallel'              : 1, # enables ipython parallelization using multiple cores\n",
    "           'preprocess_save'       : 0, # write preprocessed data to file for later import CELL 97\n",
    "           'preprocess_load'       : 0, # load previously saved preprocessed data from file instead of db (CELL 70)\n",
    "           'remove_html'           : 1, # remove some html encodings, not needed if using data in database\n",
    "           'tokenize'              : 1, # tokenize using NLTK's sentence and word tokenizers\n",
    "           'spell_correct'         : 1, # spell correct\n",
    "           'pos_tag'               : 1, # part of speech tag\n",
    "           'only_tagged'           : 1, # only use parts of speech types listed in tag_set\n",
    "           'remove_punctuation'    : 1, # remove punctuation\n",
    "           'remove_smallwords'     : 1, # removes words with len(word) < 3\n",
    "           'alpha_only'            : 0, # use alphabetic words only (probably not necessary)\n",
    "           'analyze_spell_correct' : 1, # save text before spell correction for later analysis in CELL 98, 120, 130\n",
    "           'lowercase'             : 1, # lowercase text\n",
    "           'remove_stopwords'      : 1, # remove stopwords \n",
    "           'lemmatize'             : 1, # lemmatize text\n",
    "           'analyze_lemmatize'     : 1, # save text before lemmatization for later analysis in CELL 98, 120 \n",
    "           'lemma_dict'            : 1, # produce a dict of lemmas with words mapped to them in CELL 100 (time consuming)\n",
    "           'texts_final_load'      : 0, # load the texts_final\n",
    "           'texts_final_save'      : 0, # save the texts_final\n",
    "           \n",
    "           # Dictionary / Corpus\n",
    "           # The features below concern themselves with the creation of a dictionary and corpus.\n",
    "           \n",
    "           'dict_corpus_load'      : 0, # load the dictionary and corpus in CELL 205\n",
    "           'dict_corpus_save'      : 0, # save the dictionary created in CELL 182 and the corpus created in CELL 212\n",
    "           'tfidf'                 : 0, # TF-IDF transform corpus, otherwise leave as bag of words\n",
    "           \n",
    "           # LDA\n",
    "           # The features below concer themselves with the actual creation of the LDA model.  This is where the Pyro4\n",
    "           # framework would be used if available.\n",
    "           \n",
    "           'perplexity_search'     : 0, # enables perplexity grid search in CELL 268 rather than regular model in 270\n",
    "           'model_load'            : 0, # load the model in CELL 289\n",
    "           'model_save'            : 0, # save the model in CELL 290\n",
    "           'corpus_model_load'     : 0, # load the corpus model in CELL 293\n",
    "           'corpus_model_save'     : 0, # save the corpus model in CELL 295\n",
    "           'beta_gamma_save'       : 0, # save the LDA models beta and gamma in CELL 310\n",
    "           'model_topics_load'     : 0, # load model_topics in CELL 301\n",
    "           'model_topics_save'     : 0  # save model_topics in CELL 302\n",
    "           }\n",
    "\n",
    "# Maximum number of reviews to use if loading from the database, set to 0 to use all review data requested\n",
    "# If you are just trying out this notebook, I recommend setting this to a low number like 500, 1000, 3000, etc.\n",
    "# As if the query set in CELL 70 is \"all reviews\" or something like that, it could take many hours to complete\n",
    "# text processing and model building depending on the platform it is running on.\n",
    "MAX_NUM_REVIEWS = 500\n",
    "\n",
    "# State to get data for, should be set to a valid 2 letter state abbreviation.  If set to \"ALL\" it will pull data for all states\n",
    "# This applies for when loading data from the database\n",
    "state = \"CA\"\n",
    "\n",
    "# directory to look for data files\n",
    "data_dir = \"data\"\n",
    "\n",
    "# file root name (do not add file extension).  This is used to build almost every filename used in the notebook, whether you are\n",
    "# loading or saving files.  Its important to set it correctly if you have any of the \"save\" bits set in the feature matrix, \n",
    "# otherwise you could overwrite previously saved data.\n",
    "file_root = \"schools\"\n",
    "\n",
    "# which parts of speech tags to use when only_tagged feature enabled\n",
    "# for explaination of tags http://bulba.sdsu.edu/jeanette/thesis/PennTags.html\n",
    "tag_set = ['NN','NNS']\n",
    "\n",
    "# our stopsets\n",
    "custom_stopset = ['school','teachers','students','children','parents','kids','child','parent','son','sons','student','schools',\n",
    "                  'daughter','daughters',\"I'm\",\"i'm\",\"I've\",\"i've\",'teacher','principal','principle','mr','Mr','ms','Mrs',\n",
    "                  'dint','kindergarten','would','went','get','one','ones','st','make','year','years','aw','grandson','ma',\n",
    "                  \"La's\",'la','way','thing','lot','everybody','everyone','great','excellent','part','hill','valley','den',\n",
    "                  'alto','crest','wood','land','love','amazing','okay']\n",
    "\n",
    "stopset_unigram = set(nltk.corpus.stopwords.words('english'))\n",
    "stopset_unigram = stopset_unigram.union(set(custom_stopset))\n",
    "stopset_bigrams = []\n",
    "\n",
    "# number of topics to use in LDA model.  This is set hear as opposed to just in CELL 270 where the model runs, because it is used\n",
    "# in the creation of many filenames which can be saved/loaded (set in feature dict) that are specific to the number of \n",
    "# topics of the model.\n",
    "num_topics = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 25\n",
    "# summary statistics\n",
    "def summary_stats(texts):\n",
    "    \"\"\"\n",
    "    Produces Summary Statistics on the texts provided\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : A tokenized set of documents (list of lists)\n",
    "    \"\"\"\n",
    "    token_count = defaultdict(int)\n",
    "    for word in itertools.chain.from_iterable(texts):\n",
    "        token_count[word] += 1\n",
    "    print \"%d words appear in the text just 1 time\" % sum([1 for word in token_count if token_count[word] == 1])\n",
    "    print \"%d words appear in the text just 2 times\" % sum([1 for word in token_count if token_count[word] == 2])\n",
    "    n, min_max, mean, var, skew, kurt = sp.stats.describe(token_count.values())\n",
    "    print(\"Number of unique words: {0:d}\".format(n))\n",
    "    print(\"Minimum freq: {0:8d} Maximum freq: {1:8d}\".format(min_max[0], min_max[1]))\n",
    "    print(\"Mean: {0:8.2f}\".format(mean))\n",
    "    print(\"Std. deviation : {0:8.6f}\".format(sp.std(token_count.values())))\n",
    "    print(\"Variance: {0:8.2f}\".format(var))\n",
    "    print(\"Skew : {0:8.2f}\".format(skew))\n",
    "    print(\"Kurtosis: {0:8.2f}\".format(kurt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 30\n",
    "# MySQL functions\n",
    "\n",
    "\"\"\"\n",
    "class mysql_ops\n",
    "\n",
    "functions:\n",
    "\n",
    "    mysql_connect - connects to our MySQL db\n",
    "    mysql_disconnect - disconnects\n",
    "    get_mysql_data - generic retrieval function that invokes a stored procedure returning a denormalized df combining school and review data\n",
    "    \n",
    "    retrieval methods (commented inline)\n",
    "    \n",
    "\"\"\"\n",
    "class mysql_ops(object):\n",
    "    \n",
    "    # returns a connection to our MySQL db\n",
    "    @classmethod\n",
    "    def mysql_connect(self):\n",
    "        \n",
    "        cnx = pymysql.connect(host='cs109instance.ccikshmkulj7.us-east-1.rds.amazonaws.com', \n",
    "                               port=3306, \n",
    "                               user='michael', \n",
    "                               passwd='michael', \n",
    "                               db='cs109gs')\n",
    "        this_cursor = cnx.cursor(pymysql.cursors.DictCursor)\n",
    "        return cnx, this_cursor\n",
    "    \n",
    "    # close cursor and MySQL connection\n",
    "    @classmethod\n",
    "    def mysql_disconnect(self, cnx, cursor):\n",
    "        cursor.close()\n",
    "        cnx.close() # close db connection\n",
    "        \n",
    "    # get data from mysql db\n",
    "    def get_mysql_data(self, sql, data_id1=None, data_id2=None):\n",
    "        \n",
    "        cnx, cursor = self.mysql_connect()\n",
    "        \n",
    "        if data_id2:\n",
    "            cursor.callproc(sql, (data_id1, data_id2))\n",
    "        elif data_id1:\n",
    "            cursor.callproc(sql, (data_id1,))\n",
    "        else:\n",
    "            cursor.callproc(sql)\n",
    "        \n",
    "        if (cursor.rowcount > 0):\n",
    "            df = pd.DataFrame(cursor.fetchall())\n",
    "            self.mysql_disconnect(cnx, cursor)# close db connection\n",
    "            return df\n",
    "        else:\n",
    "            self.mysql_disconnect(cnx, cursor)# close db connection\n",
    "            print \"no rows returned\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # return all reviews for one school gsid (denormalized, i.e. with the associated school data repeated per review)\n",
    "    def get_reviews_gsid(self, gsid):\n",
    "        sql = 'school_reviews_by_gsid'\n",
    "        return self.get_mysql_data(sql, gsid)\n",
    "     \n",
    "    # return ethnic composition data for one school gsid \n",
    "    def get_race_gsid(self, gsid):\n",
    "        sql = 'school_race_by_gsid'\n",
    "        return self.get_mysql_data(sql, gsid)\n",
    "    \n",
    "    # return GS census data for one school gsid \n",
    "    def get_census_gsid(self, gsid):\n",
    "        sql = 'school_census_by_gsid'\n",
    "        return self.get_mysql_data(sql, gsid)\n",
    "    \n",
    "    # return GS test results data for one school gsid \n",
    "    def get_results_gsid(self, gsid):\n",
    "        sql = 'school_results_by_gsid'\n",
    "        return self.get_mysql_data(sql, gsid)\n",
    " \n",
    "    # return all reviews for one school ncesid (denormalized)\n",
    "    def get_reviews_ncesid(self, ncesid):\n",
    "        sql = 'school_reviews_by_ncesid'\n",
    "        return self.get_mysql_data(sql, ncesid)\n",
    "    \n",
    "    # return all reviews for one school ncesid \n",
    "    def get_reviews_districtncesid(self, districtncesid):\n",
    "        sql = 'school_reviews_by_districtncesid'\n",
    "        return self.get_mysql_data(sql, districtncesid)\n",
    "    \n",
    "    # return all reviews for one state \n",
    "    def get_reviews_state(self, state):\n",
    "        sql = 'school_reviews_by_state'\n",
    "        return self.get_mysql_data(sql, state)\n",
    "    \n",
    "     # return all census data for one state \n",
    "    def get_census_state(self, state):\n",
    "        sql = 'school_census_by_state'\n",
    "        return self.get_mysql_data(sql, state)\n",
    "    \n",
    "     # return all test results for one state \n",
    "    def get_results_state(self, state):\n",
    "        sql = 'school_results_by_state'\n",
    "        return self.get_mysql_data(sql, state)\n",
    "    \n",
    "     # return all test results for one state and year\n",
    "    def get_results_state_year(self, state, year):\n",
    "        sql = 'school_results_by_state_year'\n",
    "        return self.get_mysql_data(sql, state, year)\n",
    "    \n",
    "    # return all reviews for one county\n",
    "    def get_reviews_county(self, state, county):\n",
    "        sql = 'school_reviews_by_county'\n",
    "        return self.get_mysql_data(sql, state, county)\n",
    "    \n",
    "    # return all reviews containing text string\n",
    "    def get_reviews_string(self, text_string):\n",
    "        sql = 'school_reviews_by_string'\n",
    "        return self.get_mysql_data(sql, text_string)\n",
    "    \n",
    "    # note the psql syntax: returns all schools / reviews in db \n",
    "    # CAREFUL WITH THIS ONE:  IT WILL TAKE A LONG TIME AND RETURN EVERYTHING!\n",
    "    def get_all_reviews(self):\n",
    "        sql = 'school_reviews' \n",
    "        return self.get_mysql_data(sql)\n",
    "        #df = psql.frame_query(sql, cn)\n",
    "        #cn = cnx.mysql_disconnect()\n",
    "        #return df\n",
    "    \n",
    "    #return all cities and towns for a given state\n",
    "    def get_cities_towns_by_state(self, state):\n",
    "        sql = 'get_cities_towns_by_state' \n",
    "        return self.get_mysql_data(sql, state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 50\n",
    "# Spell Checker Class\n",
    "\n",
    "# SpellingReplacer class taken from Python Text Processing with NLTK 2.0 Cookbook\n",
    "# It will return the word if its in the dictionary, otherwise return the closest word it can match if <= self.max_dist\n",
    "# Otherwise, it will return the word\n",
    "class SpellingReplacer(object):\n",
    "    \n",
    "    def __init__(self, dict_name='en_US', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = 2\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 60\n",
    "# We instantiate a SpellingReplacer, and do a quick test\n",
    "if(feature['spell_correct']):\n",
    "    replacer = SpellingReplacer()\n",
    "    replacer.replace('cookbok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 70\n",
    "# if preprocess_load is set get reviews as well as other data from pre-saved files\n",
    "# otherwise get reviews data either by db\n",
    "\n",
    "if(feature['preprocess_load']):\n",
    "    filename = data_dir + '/' + file_root + '-reviews_indexes.pickle'\n",
    "    with open(filename) as f:\n",
    "        reviews_indexes = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(reviews_indexes))\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-reviews.pickle'\n",
    "    with open(filename) as f:\n",
    "        reviews = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(reviews))\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-school_names.pickle'\n",
    "    with open(filename) as f:\n",
    "        school_names = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(school_names))\n",
    "    \n",
    "    filename = data_dir + '/' + file_root + '-texts.pickle'\n",
    "    with open(filename) as f:\n",
    "        texts = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(texts))\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-texts_uncorrected.pickle'\n",
    "    with open(filename) as f:\n",
    "        texts_uncorrected = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(texts_uncorrected))\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-texts_unlemmatized.pickle'\n",
    "    with open(filename) as f:\n",
    "        texts_unlemmatized = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(texts_unlemmatized))\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-texts_pos.pickle'\n",
    "    with open(filename) as f:\n",
    "        texts_pos = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(texts_pos))\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-lemma_dict.pickle'\n",
    "    with open(filename) as f:\n",
    "        lemma_dict = pickle.load(f)\n",
    "    print \"Loaded file %s of size %d\" % (filename,len(lemma_dict))\n",
    "\n",
    "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "    # instance mysql_ops\n",
    "    cnx = mysql_ops()\n",
    "    \n",
    "    # Here is where we set what query we wish to use.  It can be one that involves a constrain using MAX_NUM_REVIEWS or not.\n",
    "    # You can add whatever query you like, a full list are available in CELL 30.  Below just shows \"get_all_reviews()\" and\n",
    "    # \"get_all_reviews_state\" as those would likely be the most commonly used with this notebook.\n",
    "    if(MAX_NUM_REVIEWS == 0):\n",
    "        if((state == \"ALL\") or (state == \"all\")):\n",
    "            reviews = cnx.get_all_reviews()[['id','nces_code','universal_id','postdate','name','reviews']]\n",
    "        else:\n",
    "            reviews = cnx.get_reviews_state(state)[['id','nces_code','universal_id','postdate','name','reviews']]\n",
    "    else:\n",
    "        if((state == \"ALL\") or (state == \"all\")):\n",
    "            reviews = cnx.get_all_reviews()[['id','nces_code','universal_id','postdate','name','reviews']][:MAX_NUM_REVIEWS]\n",
    "        else:\n",
    "            reviews = cnx.get_reviews_state(state)[['id','nces_code','universal_id','postdate','name','reviews']][:MAX_NUM_REVIEWS]\n",
    "            \n",
    "    # We save off the indexes of each review so we can correlate everything later if needed\n",
    "    reviews_indexes = reviews[['id','nces_code','universal_id','postdate']]\n",
    "    \n",
    "    # This is a list of school names, which we immediately reduce to a set().  Its only being used to create bigrams from\n",
    "    # which will them be added to a special stopset for bigrams.  Here they are unprocessed by after CELL 90 they will be \n",
    "    # very different looking, and very sparse, as most will have been removed during pre-processing (most parts of school names\n",
    "    # are either stopped in our pre-processing or are not parts of speech that we decide to keep, nouns for example)\n",
    "    school_names = list(set(reviews['name']))\n",
    "    \n",
    "    # This is the core of what we are processing and building our model on.  The unprocessed data from the database.\n",
    "    reviews = reviews['reviews']\n",
    "    print \"Ingested %d documents from database\" % len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 75\n",
    "# setup ipython for parallel processing\n",
    "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "    from IPython.parallel import Client\n",
    "\n",
    "    # first start an ipython cluster on your machine.  For example, for 4 cores:\n",
    "    #    ipcluster start -n 4 \n",
    "\n",
    "    # Setup client instance\n",
    "    rc = Client()\n",
    "    print \"Discovered %d cores\" % len(rc)\n",
    "    rc.ids\n",
    "\n",
    "    # we will use a DirectView object for direct execution across all cores\n",
    "    # all your cores are belong to us\n",
    "    dv = rc[:]\n",
    "\n",
    "    # We will block on all executions\n",
    "    dv.block=True\n",
    "\n",
    "    dv.scatter('partition_ids', range(len(rc)))\n",
    "    %px print(partition_ids)\n",
    "    %px partition_id = partition_ids[0]\n",
    "    %px print(partition_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 80\n",
    "# remove punctuation, remove stopwords, remove words < 3, remove non alpha words, spell correct, lemmatize\n",
    "# you need to comment out the below decorator if you don't want to run parallel\n",
    "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "\n",
    "    @dv.remote(block=True)\n",
    "    def clean_data():\n",
    "    \n",
    "        replacer = SpellingReplacer()\n",
    "\n",
    "        texts = []\n",
    "        texts_uncorrected = []\n",
    "        texts_unlemmatized = []\n",
    "        texts_pos = []\n",
    "        lemma_dict = defaultdict(set)\n",
    "        counter = 0\n",
    "    \n",
    "        # we need to strip escape characters so we compile a pattern\n",
    "        hexchars = re.compile('\\\\\\\\x(\\w{2})')\n",
    "        \n",
    "        # Instantiate a WordNet Lemmatizer\n",
    "        wnl = WordNetLemmatizer()\n",
    "        \n",
    "        # convert penn_tag to morphy_tag used by WordNet Lemmatizer\n",
    "        # http://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk\n",
    "        morphy_tag = {'NN':wordnet.NOUN,'JJ':wordnet.ADJ,'VB':wordnet.VERB,'RB':wordnet.ADV}\n",
    "        \n",
    "        print \"Starting to process %d documents\" % len(data_to_clean)\n",
    "    \n",
    "        for review_text in data_to_clean:\n",
    "            \n",
    "            # print result to stdout, note that when using parallel stdout is not sent to the Client\n",
    "            if(not feature['parallel']):\n",
    "                counter += 1\n",
    "                if ((counter % 1000)==0):\n",
    "                    print \"%d documents processed\" % counter\n",
    "                sys.stdout.flush()\n",
    "                          \n",
    "            # If its empty, impute \"the\" which is ignored anyways.  This needs to be done vs. skipping in order\n",
    "            # to keep our indexes 1:1 across all texts for analysis\n",
    "            if review_text == None:\n",
    "                review_text = \"the\"\n",
    "        \n",
    "            # remove html encodings\n",
    "            if(feature['remove_html']):\n",
    "                review_text = review_text.replace('&amp;','').replace('&lt;','').replace('&gt;','').replace('&quot;','').replace('&#039;','').replace('&#034;','')\n",
    "                review_text = re.sub(hexchars, '', review_text.encode(\"string-escape\"))\n",
    "          \n",
    "            # tokenize, you don't want to turn this off\n",
    "            if(feature['tokenize']):\n",
    "                review_text = [word for sent in sent_tokenize(review_text) for word in word_tokenize(sent)]\n",
    "            else:\n",
    "                review_text = [word for word in review_text.split()]\n",
    "            \n",
    "            # Below line is just to store uncorrected text so we can check spelling routines later\n",
    "            if(feature['analyze_spell_correct'] and feature['spell_correct']):\n",
    "                texts_uncorrected.append(review_text)\n",
    "            \n",
    "            # Spell correction using the Enchant library\n",
    "            # The corrected words that are returned may be capitalized as part of the correction, or even have\n",
    "            # things like apostrophes added.  We will just leave these in place for now as this does not effect\n",
    "            # the data quality. What is important is at this point the data has been de-duplicated, corrected and\n",
    "            # is consistant.  So instead of \"california\", \"California\", \"Califonria\", it may all be just \"California\"\n",
    "            # Note that spell correction DOES remove punctuation!\n",
    "            if(feature['spell_correct']):\n",
    "                review_text =  [word for words in review_text for word in replacer.replace(words).split()]\n",
    "                   \n",
    "            # get parts of speech\n",
    "            if(feature['pos_tag']):\n",
    "                review_text_pos = [word for word in nltk.pos_tag(review_text)]\n",
    "                texts_pos.append(review_text_pos)\n",
    "            \n",
    "                # Remove anything that is not in our tag_set\n",
    "                if(feature['only_tagged']):\n",
    "                    # review_text = [word[0] for word in review_text_pos if word[1] in tag_set]\n",
    "                    review_text = [word for word in review_text_pos if word[1] in tag_set]\n",
    "                else:\n",
    "                    review_text = [word for word in review_text_pos]\n",
    "    \n",
    "            # remove punctuation\n",
    "            if(feature['remove_punctuation']):\n",
    "                if(feature['pos_tag']):\n",
    "                    review_text = [(word[0].translate(string.maketrans(\"\",\"\"), string.punctuation),word[1]) for word in review_text]\n",
    "                else:\n",
    "                    review_text = [word.translate(string.maketrans(\"\",\"\"), string.punctuation) for word in review_text]\n",
    "                        \n",
    "            # remove smallwords\n",
    "            if(feature['remove_smallwords']):\n",
    "                if(feature['pos_tag']):\n",
    "                    review_text = [(word[0],word[1]) for word in review_text if not len(word[0]) < 3]\n",
    "                else:\n",
    "                    review_text = [word for word in review_text if not len(word) < 3]\n",
    "                \n",
    "            # Only consider alpha words (probably uncessary)\n",
    "            if(feature['alpha_only']):\n",
    "                if(feature['pos_tag']):\n",
    "                    review_text = [(word[0],word[1]) for word in review_text if word[0].isalpha()]\n",
    "                else:\n",
    "                    review_text = [word for word in review_text if word.isalpha()]\n",
    "            \n",
    "            # lowercase all text\n",
    "            if(feature['lowercase']):\n",
    "                if(feature['pos_tag']):\n",
    "                    review_text = [(word[0].lower(),word[1]) for word in review_text]\n",
    "                else:\n",
    "                    review_text = [word.lower() for word in review_text]\n",
    "            \n",
    "            # remove stopwords\n",
    "            if(feature['remove_stopwords']):\n",
    "                if(feature['pos_tag']):\n",
    "                    review_text = [word for word in review_text if not word[0] in stopset_to_use]\n",
    "                else:\n",
    "                    review_text = [word for word in review_text if not word in stopset_to_use]\n",
    "            \n",
    "            # Below line is just to store unlemmatized text so we can check statistics later\n",
    "            if((feature['analyze_lemmatize'] and feature['lemmatize']) or (feature['analyze_spell_correct'] and feature['spell_correct'])):\n",
    "                if(feature['pos_tag']):\n",
    "                    texts_unlemmatized.append([word[0] for word in review_text])\n",
    "                else:\n",
    "                    texts_unlemmatized.append(review_text)\n",
    "                    \n",
    "            # Lemmatize using the Wordnet Lemmatizer\n",
    "            # Use nltk's lemmatizer to create word stems\n",
    "            if(feature['lemmatize'] and not feature['lemma_dict']):\n",
    "                if(feature['pos_tag']):\n",
    "                    # convert penn_tag to morphy_tag used by WordNet Lemmatizer\n",
    "                    review_text = [wnl.lemmatize(word[0],morphy_tag[word[1][:2]]) for word in review_text]\n",
    "                else:\n",
    "                    review_text = [wnl.lemmatize(word) for word in review_text]\n",
    "                \n",
    "            # creating a dictionary of lemmas can take a while, you may wish to first try on a small set by setting MAX_NUM_REVIEWS\n",
    "            # in CELL 0\n",
    "            if(feature['lemmatize'] and feature['lemma_dict']):\n",
    "                lemma_text = []\n",
    "                for word in review_text:\n",
    "                    if(feature['pos_tag']):\n",
    "                        orig_word = word[0]\n",
    "                        lemma = wnl.lemmatize(word[0],morphy_tag[word[1][:2]])\n",
    "                    else:\n",
    "                        orig_word = word\n",
    "                        lemma = wnl.lemmatize(word)\n",
    "                        \n",
    "                    # if word doesn't match its lemma then add this word to our lemma_dict\n",
    "                    if lemma != orig_word:\n",
    "                        lemma_dict[lemma].add(orig_word)\n",
    "                        \n",
    "                    lemma_text.append(lemma)\n",
    "                \n",
    "                review_text = lemma_text\n",
    "    \n",
    "            # remove stopwords and smallwords a second time, to make sure none were introduced from lemmatization \n",
    "            if(feature['lemmatize']):\n",
    "                if(feature['remove_smallwords']):\n",
    "                    review_text = [word for word in review_text if not len(word) < 3]\n",
    "    \n",
    "                if(feature['remove_stopwords']):\n",
    "                    review_text = [word for word in review_text if not word in stopset_to_use]\n",
    "    \n",
    "                        \n",
    "            # if pos_tag is turned on but lemma is not, then our text still has tags and we need to remove them\n",
    "            if(feature['pos_tag'] and not feature['lemmatize']):\n",
    "                review_text = [word[0] for word in review_text]\n",
    "            \n",
    "            # add to our list\n",
    "            texts.append(review_text)\n",
    "        return texts, texts_uncorrected, texts_unlemmatized, texts_pos, lemma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 85\n",
    "# setup clean_data for parallel processing\n",
    "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "\n",
    "    # We sychronize our imports to our remote workers, as they have seperate environments than ours\n",
    "    # these are just the imports that are needed for clean_data()\n",
    "    with dv.sync_imports():\n",
    "        import nltk\n",
    "        import sys\n",
    "        import string\n",
    "        import enchant\n",
    "        import re\n",
    "        from collections import defaultdict\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "        from nltk.metrics import edit_distance\n",
    "        from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# CELL 90\n",
    "\n",
    "# %%time reports Wall time: 4.19 s for my Quad Core 2.66Ghz mac pro in parallel mode\n",
    "# %%time reports Wall time: 1min 2s for Amazon instance using ALL reviews data ~900k\n",
    "\n",
    "# get school names and create bigrams from them, then add those bigrams to our bigram stopset\n",
    "\n",
    "# We clean our school_names the exact same way we clean other data, this must be done for them be used\n",
    "# properly as stop words\n",
    "\n",
    "# You will NOT see stdout from the workers if using parallel processing\n",
    "\n",
    "# The commands below make use of the global name space as opposed to passing variables to the function. The nature of ipythons\n",
    "# paralell architecture is that it basically relies on global variables in its own environment.  We rename them here to \n",
    "# keep things organized\n",
    "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "    \n",
    "    data_to_clean = school_names\n",
    "    stopset_to_use = stopset_unigram\n",
    "    \n",
    "    # copy the data_to_clean and stopset_to_use into name space of remote workers\n",
    "    if(feature['parallel']):\n",
    "        \n",
    "        # We use ipythons parallel processing \"scatter\" to shard \"reviews\" accross our cores\n",
    "        dv.scatter('data_to_clean',data_to_clean)\n",
    "        \n",
    "        # We copy the objects feature, stopset, tag_set as well as the Class SpellingReplacer to our remote workers\n",
    "        dv['feature'] = feature\n",
    "        dv['stopset_to_use'] = stopset_to_use\n",
    "        dv['tag_set'] = tag_set\n",
    "        dv['SpellingReplacer'] = SpellingReplacer\n",
    "        \n",
    "        # Show size of entire data being sent for processing\n",
    "        print \"Total data size: %d\" % len(data_to_clean)\n",
    "        \n",
    "        # Show the shard size of each remote worker\n",
    "        print \"Sharded data size for each worker\"\n",
    "        %px print len(data_to_clean)\n",
    "    \n",
    "        # clean the school_names\n",
    "        result = clean_data()\n",
    "    else:\n",
    "        # We are not running parallel and we only care about the first text returned from clean_data()\n",
    "        school_names = clean_data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 91\n",
    "# Merge Parallel Data back from remote workers back into one set for school_names\n",
    "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "    school_names = []\n",
    "\n",
    "    for worker in result:\n",
    "        school_names.extend(worker[0])\n",
    "\n",
    "    print \"Total data size after merge: %d\" % len(school_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Richening the text with bigrams\n",
    "\n",
    "We created bigrams from the above text in `CELL 110` and included those as well.  This brought about interesting bigrams that would frequently appear, such as *test scores*,  *front office*, *music program*, *field trip*, etc.  Once again, these were just built off of nouns.  The bigrams were added back to our processed document so that we ended up with each review being a list of unigrams and bigrams of that review.  We created a stopset here in `CELL 93` using the school names, so that these could be stopped from any bigrams that were created.  This is because the school names themselves were very frequent in bigrams and we were not concerned with the names of the school in doing topic analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 93\n",
    "# Create stopset_bigrams from bigrams created using school names for creation of a bigram stopset\n",
    "\n",
    "# We create bigrams from the school_names, union with the original stopset to create our final bigram stopset\n",
    "\n",
    "# We only stop bigrams made from school names.  Not unigrams.  This is because there are too many words potentially used in school \n",
    "# names which we would not want to stop.  For example the word \"Art\", or \"Science\" or any number of other words.  When discussing \n",
    "# a school in a review however, reviewers repeatedly mention school names, for example if the school name were \"Glenn Brook Middle\n",
    "# School\" they may constantly mention \"Glenn Brook\".......this gets rid of it.  The reality is, when working with a tag_set of just\n",
    "# nounds (NN/NNS), most names of schools are removed from the text anyways, as they are typically Proper Nouns (NNP) or adjectives\n",
    "# and never survive further along to give us any trouble.  However having this bigram creation and stoplist is very useful if you \n",
    "# are NOT limiting to just nouns, so it give us great flexibility.\n",
    "if(not feature['texts_final_load']):\n",
    "    school_names_bigram = [[\" \".join(bigram) for bigram in bigrams(text)] for text in school_names]\n",
    "    school_names_bigram = list(itertools.chain.from_iterable(school_names_bigram))\n",
    "    stopset_bigrams = set(school_names_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# CELL 94 \n",
    "# collect optional uncorrected, unlemmatized, pos tagged and lemma dict texts for later analysis\n",
    "\n",
    "# time 1m 56s for 3000 reviews on my 2.66Ghz quad core xeon mac pro\n",
    "# time 25m for 37349 reviews on my 2.66Ghz quad core xeon mac pro\n",
    "# time 18m 18s for 37349 reviews on my 2.66Ghz quad core xeon mac pro with 8 virtual cores HyperThreading\n",
    "# time 31m 20s for 166610 CA reviews on Amazon EC2 16 hyperthreaded server\n",
    "# time 2h 15m for 890000 reviews of all schools on Amazon EC2 32 hyperthreaded server \n",
    "\n",
    "# get our cleaned reviews\n",
    "# We can do this using parallel processing or not\n",
    "# You will NOT see stdout from the workers if using parallel processing\n",
    "\n",
    "# The commands below make use of the global name space in each worker as opposed to passing variables to the function. The nature of\n",
    "# ipythons paralell architecture is that it basically utilizes separate python processes, each with their own global namespace \n",
    "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "    \n",
    "    data_to_clean = reviews\n",
    "    stopset_to_use = stopset_unigram\n",
    "    \n",
    "    # copy the data_to_clean and stopset_to_use into name space of remote workers\n",
    "    if(feature['parallel']):\n",
    "        \n",
    "        # We use ipythons parallel processing \"scatter\" to shard \"reviews\" accross our cores\n",
    "        dv.scatter('data_to_clean',data_to_clean)\n",
    "        \n",
    "        # We copy the objects feature and stopset, as well as the Class SpellingReplacer to our remote workers\n",
    "        dv['feature'] = feature\n",
    "        dv['stopset_to_use'] = stopset_to_use\n",
    "        dv['tag_set'] = tag_set\n",
    "        dv['SpellingReplacer'] = SpellingReplacer\n",
    "        \n",
    "        # Show size of entire data being sent for processing\n",
    "        print \"Total data size: %d\" % len(data_to_clean)\n",
    "        \n",
    "        # Show the shard size of each remote worker\n",
    "        print \"Sharded data size for each worker\"\n",
    "        %px print len(data_to_clean)\n",
    "    \n",
    "        # clean the reviews\n",
    "        result = clean_data()\n",
    "    else:\n",
    "        # run non-parallel\n",
    "        texts,texts_uncorrected,texts_unlemmatized,texts_pos,lemma_dict = clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 95\n",
    "# Diagnostics to to make sure all of our data is here.  There should be no drift in the lengths of texts.\n",
    "\n",
    "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "\n",
    "    texts_size = 0\n",
    "    texts_uncorrected_size = 0\n",
    "    texts_unlemmatized_size = 0\n",
    "    texts_pos_size = 0\n",
    "    lemma_dict_size = 0\n",
    "\n",
    "    # iterate through each worker and get lengths of the texts\n",
    "    for worker in result:\n",
    "        texts_size += len(worker[0])\n",
    "        texts_uncorrected_size += len(worker[1])\n",
    "        texts_unlemmatized_size += len(worker[2])\n",
    "        texts_pos_size += len(worker[3])\n",
    "        lemma_dict_size += len(worker[4])\n",
    "        \n",
    "if(not feature['parallel'] or feature['preprocess_load']):\n",
    "    texts_size = len(texts)\n",
    "    texts_uncorrected_size = len(texts_uncorrected)\n",
    "    texts_unlemmatized_size = len(texts_unlemmatized)\n",
    "    texts_pos_size = len(texts_pos)\n",
    "    lemma_dict_size = len(lemma_dict)\n",
    "\n",
    "if(not feature['texts_final_load']):\n",
    "    print \"original reviews size %d\" % len(reviews)\n",
    "    print \"combined size of remote workers texts_pos %d\" % texts_pos_size\n",
    "    print \"combined size of remote workers texts_uncorrected %d\" % texts_uncorrected_size\n",
    "    print \"combined size of remote workers texts_unlemmatized %d\" % texts_unlemmatized_size\n",
    "    print \"combined size of remote workers texts %d\" % texts_size\n",
    "    # sum of len of seperate dictionaries is not necessarily equal to their original/merged size\n",
    "    print \"combined size of remote workers lemma_dict %d\" % lemma_dict_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# CELL 96\n",
    "# Merge Parallel Data back from remote workers back into one set\n",
    "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
    "    texts = []\n",
    "    texts_uncorrected = []\n",
    "    texts_unlemmatized = []\n",
    "    texts_pos = []\n",
    "    lemma_dict = defaultdict(set)\n",
    "\n",
    "    # iterate through each worker and merge the texts back together\n",
    "    for worker in result:\n",
    "        texts += worker[0]\n",
    "        texts_uncorrected += worker[1]\n",
    "        texts_unlemmatized += worker[2]\n",
    "        texts_pos += worker[3]\n",
    "        for k in worker[4]:\n",
    "            lemma_dict[k] = lemma_dict[k].union(worker[4][k])\n",
    "    print \"Total data size after merge: %d\" % len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 97\n",
    "# save data \n",
    "# write reviews, school_names, texts, texts_uncorrected, texts_unlemmatized, texts_pos, lemma_dict\n",
    "# out to a file using pickle.  This is so we don't have to re-run pre-processing later if we want to work on the same data\n",
    "if(feature['preprocess_save']):\n",
    "    filename = data_dir + '/' + file_root + '-reviews_indexes.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(reviews_indexes))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(reviews_indexes, f)\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-reviews.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(reviews))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(reviews, f)\n",
    "\n",
    "    filename = data_dir + '/' + file_root + '-school_names.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(school_names))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(school_names, f)\n",
    "        \n",
    "    filename = data_dir + '/' + file_root + '-texts.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(texts))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(texts, f)\n",
    "        \n",
    "    filename = data_dir + '/' + file_root + '-texts_uncorrected.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(texts_uncorrected))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(texts_uncorrected, f)\n",
    "        \n",
    "    filename = data_dir + '/' + file_root + '-texts_unlemmatized.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(texts_unlemmatized))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(texts_unlemmatized, f)\n",
    "        \n",
    "    filename = data_dir + '/' + file_root + '-texts_pos.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(texts_pos))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(texts_pos, f)\n",
    "        \n",
    "    filename = data_dir + '/' + file_root + '-lemma_dict.pickle'\n",
    "    print \"Writing file %s of size %d\" % (filename,len(lemma_dict))\n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(lemma_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 98\n",
    "# Look at the text or analyze the text processing pipeline for a single review\n",
    "\n",
    "def get_review_data(review_num):\n",
    "    print \"\\nData from reviews for review number %d\" % review_num\n",
    "    print \"GSID: %s NCES Code: %s    Universal ID: %s\" % (reviews_indexes['id'][review_num],reviews_indexes['nces_code'][review_num],reviews_indexes['universal_id'][review_num])\n",
    "    print \"\\n\"\n",
    "    print reviews[review_num]\n",
    "    \n",
    "    if(feature['analyze_spell_correct'] and feature['spell_correct']):\n",
    "        print \"\\nData from texts_uncorrected for review number %d\" % review_num\n",
    "        print texts_uncorrected[review_num]\n",
    "\n",
    "    if(feature['pos_tag']):\n",
    "        print \"\\nData from texts_pos for review number %d\" % review_num\n",
    "        print texts_pos[review_num]\n",
    "\n",
    "    if((feature['analyze_lemmatize'] and feature['lemmatize']) or (feature['analyze_spell_correct'] and feature['spell_correct'])):\n",
    "        print \"\\nData from texts_unlemmatized for review number %d\" % review_num\n",
    "        print texts_unlemmatized[review_num]\n",
    "\n",
    "    print \"\\nData from texts for review number %d\" % review_num\n",
    "    print texts[review_num]\n",
    "  \n",
    "if(not feature['texts_final_load']):\n",
    "    print \"Length of ingested reviews: %d\" % len(reviews)\n",
    "    print \"Length of texts_pos: %d\" % len(texts_pos)\n",
    "    print \"Length of texts_uncorrected: %d\" % len(texts_uncorrected)\n",
    "    print \"Length of texts_lemmatized: %d\" % len(texts_unlemmatized)\n",
    "    print \"Length of texts: %d\" % len(texts)\n",
    "    get_review_data(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 99\n",
    "# view descriptive statistics on our texts\n",
    "# Very interesting information here, over half the text is words that appear just 1 or 2 times!\n",
    "if(not feature['texts_final_load']):\n",
    "    if((feature['analyze_lemmatize'] and feature['lemmatize']) or (feature['analyze_spell_correct'] and feature['spell_correct'])):\n",
    "        print \"Descriptive statistics for texts_uncorrected\"\n",
    "        summary_stats(texts_uncorrected)\n",
    "    if(feature['analyze_spell_correct'] and feature['spell_correct']):\n",
    "        print \"\\nDescriptive statistics for texts_unlemmatized (after spell correction, removal of POS not in our tag_set, removal of stopwords, etc. but before lemmatization)\"\n",
    "        summary_stats(texts_unlemmatized)\n",
    "    print \"\\nDescriptive statistics for texts (lemmatized and done with pre-processing)\"\n",
    "    summary_stats(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 100\n",
    "# This cell just prints some intresting information about the lemmatization\n",
    "\n",
    "if(not feature['texts_final_load']):\n",
    "    if(feature['lemmatize'] and feature['lemma_dict']):\n",
    "        lemma_list = []\n",
    "        lemma_total = 0\n",
    "        for k,v in lemma_dict.iteritems():\n",
    "            lemma_list.append((k,list(v)))\n",
    "            lemma_total += len(v)+1\n",
    "        print \"there were %d words that were lemmatized down to a total of %d lemmas\" % (lemma_total,len(lemma_dict))\n",
    "        lemma_list = sorted(lemma_list, key = lambda x: len(x[1]), reverse=True)\n",
    "        for lemma in lemma_list:\n",
    "            print lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 110\n",
    "# Create texts_bigrams\n",
    "\n",
    "# Our final text is actually a creation of the orignal unigrams we decided to keep and their bigrams.\n",
    "if(not feature['texts_final_load']):\n",
    "    texts_bigrams = [[\" \".join(bigram) for bigram in bigrams(text)] for text in texts]\n",
    "    texts_bigrams = [[bigram for bigram in text if bigram not in stopset_bigrams] for text in texts_bigrams]\n",
    "    # texts_bigrams\n",
    "    \n",
    "# Look at top 10 Bigrams (from our unigram text)\n",
    "# bcf = BigramCollocationFinder.from_words(itertools.chain.from_iterable(texts))\n",
    "# bcf.nbest(BigramAssocMeasures.likelihood_ratio, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 150\n",
    "# examining the most frequent words, to see at what point trash is introduced\n",
    "\n",
    "def frequent_tokens(texts,topn):\n",
    "    \"\"\"\n",
    "    Prints frequency counts of tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : tokenized, list of lists\n",
    "        the text to analyze\n",
    "    topn : int\n",
    "        print top n frequency counts\n",
    "    \"\"\"\n",
    "    fdist = nltk.FreqDist(list(itertools.chain.from_iterable(texts)))\n",
    "    for word,count in fdist.items()[:topn]:\n",
    "        print count, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 151\n",
    "# topn most frequent unigrams\n",
    "if(not feature['texts_final_load']):\n",
    "    frequent_tokens(texts,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 152\n",
    "# topn most frequent bigrams\n",
    "if(not feature['texts_final_load']):\n",
    "    frequent_tokens(texts_bigrams, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 160\n",
    "# Load texts_final\n",
    "if(feature['texts_final_load']):\n",
    "    with open(data_dir + '/' + file_root + '-texts_final' + '.pickle') as f:\n",
    "        texts_final = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 170\n",
    "# Stitch unigrams and bigrams together in a new text texts_combined\n",
    "\n",
    "if(not feature['texts_final_load']):\n",
    "    texts_combined = []\n",
    "    for index,text in enumerate(texts):\n",
    "        texts_combined.append(text+texts_bigrams[index])\n",
    "    \n",
    "    print len(texts_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 175\n",
    "# Choose which text we will be processing with our model:\n",
    "#    unigrams = texts\n",
    "#    bigrams  = texts_bigrams\n",
    "#    unigrams + bigrams = texts_combined\n",
    "\n",
    "if(not feature['texts_final_load']):\n",
    "    # texts_final = texts\n",
    "    # texts_final = texts_bigrams\n",
    "    texts_final = texts_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 178\n",
    "# Save texts_final\n",
    "\n",
    "if(feature['texts_final_save']):\n",
    "    with open(data_dir + '/' + file_root + '-texts_final' + '.pickle', 'w') as f:\n",
    "        pickle.dump(texts_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideration of various models\n",
    "\n",
    "Various models were considered and experimented with using a variety of tools.  In python [LSI](http://en.wikipedia.org/wiki/Latent_semantic_indexing), [LDA](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) and [HDP](http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process) were all experimented with.  Ultimately we chose to work with the [gensim implementation of LDA](http://radimrehurek.com/gensim/models/ldamodel.html).  There were a number of reasons gensim was chosen:\n",
    "\n",
    "- Gensim is a mature project with over 5 years of being vetted\n",
    "- Gensim is specifically evolved around the creation of topic models\n",
    "- Gensim runs on python, a language that could deal with large amounts of data\n",
    "- Gensim supports a distributed architecture, allowing us to parallelize its operations \n",
    "\n",
    "## LDA Workflow\n",
    "\n",
    "LDA operates over a bag of words (bow).  The bow is created from first creating a dictionary of our final processed text, and then passing the text to the dictionary to create a bow.  LDA they takes the dictionary and bow and builds the model.  The end result is that topics are assigned to each document.\n",
    "\n",
    "![Alt text](files/img_bf_classification.png)\n",
    "\n",
    "## Dictionary and Corpus Creation\n",
    "\n",
    "Our dictionary was created from the output of our text processing.  After the dictionary was created, we removed infrequent and frequent words by using the `dictionary.filter_extremes()` method.  We removed all words that did not appear in at least 5 documents, removed all words that appeared in more than 60% of the documents.  There are various ways in which you can present a corpus.  LDA relies on a bow representation.  We experimented with using a TD-IDF transform, but doing so is not consistant with the model of LDA.\n",
    "\n",
    "## Bug discovery and fix in Gensim\n",
    "\n",
    "Gensim is a project that has existed for over 5 years.  The code is very solid and trusted.  In running our grid searches however, we discovered a bug that was previously unknown.  The bug was not only identified as a bug, but the true root cause was found, and solution recommended to the author, which was implemented in its [pyro threads](https://github.com/piskvorky/gensim/tree/pyro_threads) branch.  The bug was related to how gensim was allocating threads.  It was not releasing the threads when it was finished with the model.  Therefore, creation of new models, for example during an iteration process of grid searching, would exhaust threads.  The design pattern deployed by gensim was very quick and dirty, it did not stick to the traditional producer/consumer pattern that is common to RMI models.  It had a context in which it was continously blocking until new data would arrive.  This was modified so that it would wait for a signal that their was new data, sleeping and regularly checking it, which allowed for it to receive further instructions as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 180\n",
    "# create a dictionary of words and filter it\n",
    "# essentially makeing a N-D vector representation\n",
    "\n",
    "if(not feature['dict_corpus_load']):\n",
    "\n",
    "    # assigns integer id's to each word along with word counts and other statistics\n",
    "    dictionary = corpora.Dictionary(texts_final)\n",
    "\n",
    "    # filter dictionary, still need to find optimal settings default is no filtering\n",
    "    # Currently trying this at no_below=5, no_above=0.6, keep_n=None\n",
    "    # filters\n",
    "    # 1. less than no_below documents (absolute number) or\n",
    "    # 2. more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "    # after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
    "    # After the pruning, shrink resulting gaps in word ids. (this means word ids may change after gap shrinking)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.6, keep_n=None)\n",
    "    # print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 182\n",
    "# save dictionary to disk for later use\n",
    "\n",
    "if(feature['dict_corpus_save']):\n",
    "    dictionary.save(data_dir + '/' + file_root + '.dict') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 190\n",
    "# look at mappings between id's and words\n",
    "\n",
    "# print dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 200\n",
    "# test our dictionary\n",
    "\n",
    "# we take the string to lower() and split on whitespace.  If this example had more complex things like punctuation, we would\n",
    "# need to handle it like I did earlier with the full text reviews......this is just a simple example.\n",
    "# the words \"has\" and \"and\" are ignored because these are stopwords, and were filtered and never added to the dictionary \n",
    "# in the first place.  The word \"sauropod\" does not appear in the dictionary because it was not in any review we ingested.\n",
    "# id mappings below may be different from actual at this time\n",
    "# 51 is \"school\", 57 is \"students\", 61 is \"teachers\"\n",
    "# The ,1 after each id is the word count, which in this case is 2 for students and 1 for others \n",
    "# Note: now that I do dictionary filtering, the above words are filtered out, so the example is not the same as\n",
    "# what you will see below\n",
    "# new_doc = \"military organizes disgrace and sauropods\"\n",
    "# new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "# print new_vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 205\n",
    "# load a previous saved dictionary and corpus\n",
    "\n",
    "if(feature['dict_corpus_load']):\n",
    "\n",
    "    dictionary = corpora.Dictionary.load(data_dir + '/' + file_root + '.dict')\n",
    "\n",
    "    # do not re-filter if the dictionary you are loading was already previously filtered before it was saved \n",
    "    # dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "    # print dictionary\n",
    "\n",
    "    corpus_bow = corpora.MmCorpus(data_dir + '/' + file_root + '.mm')\n",
    "    # print corpus_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 210\n",
    "# our text converted to a bag of words based on our dictionary\n",
    "if(not feature['dict_corpus_load']):\n",
    "    corpus_bow = [dictionary.doc2bow(text) for text in texts_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 212\n",
    "# save corpus to disk for later use\n",
    "if(feature['dict_corpus_save']):\n",
    "    corpora.MmCorpus.serialize(data_dir + '/' + file_root + '.mm', corpus_bow) \n",
    "    # print corpus_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 230\n",
    "# How many times does a particular word appear in the corpus?\n",
    "\n",
    "# Look up the id of the token \n",
    "# id = dictionary.token2id['military']\n",
    "\n",
    "# Now get the count for that id\n",
    "# total_sum = sum(dict(doc).get(123, 0) for doc in corpus_bow)\n",
    "# print total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 240\n",
    "\n",
    "# create a TF-IDF model of our corpus\n",
    "# this model can now convert data that was represented as \"bag of words\" to the new TF-IDF representation\n",
    "# From the documentation \"Expects a bag-of-words (integer values) training corpus during initialization. During transformation, \n",
    "# it will take a vector and return another vector of the same dimensionality, except that features which were rare in the training \n",
    "# corpus will have their value increased. It therefore converts integer-valued vectors into real-valued ones, while leaving the \n",
    "# number of dimensions intact. It can also optionally normalize the resulting vectors to (Euclidean) unit length.\"\n",
    "# Note, I did not normalize here but I could.\n",
    "if(feature['tfidf']):\n",
    "    model_tfidf = models.TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 250\n",
    "# view a bow in our tfidf corpus\n",
    "# for example I take review 1\n",
    "# doc_bow = [(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 4), \n",
    "#           (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 2), \n",
    "#           (25, 1), (26, 1), (27, 2), (28, 2), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), \n",
    "#           (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), \n",
    "#           (49, 1), (50, 1), (51, 3), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 3), (58, 1), (59, 2), (60, 1), \n",
    "#           (61, 3), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 2)]\n",
    "# print model_tfidf[doc_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 260\n",
    "# transform the entire corpus to TF-IDF\n",
    "if(feature['tfidf']):\n",
    "    corpus_tfidf = model_tfidf[corpus_bow]\n",
    "\n",
    "# This can take a while to print (15m or so)\n",
    "# for doc in corpus_tfidf:\n",
    "#    print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 265\n",
    "# choose a corpus\n",
    "if(feature['tfidf']):\n",
    "    corpus = corpus_tfidf\n",
    "else:\n",
    "    corpus = corpus_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 268\n",
    "# perplexity grid search\n",
    "# we will take our documents, divide them into 80% train and 20% test.  We will measure the test data against the model built with \n",
    "# train  and calculate a perplexity measurement.  The goal will be to find the parameters which minimize perplexity.\n",
    "# Intution gathered from posts on the gensim mailing list including https://groups.google.com/forum/#!topic/gensim/tsGNoDkMY7U\n",
    "\n",
    "if(feature['perplexity_search']):\n",
    "    \n",
    "    grid = defaultdict(list)\n",
    "\n",
    "    # Choose a parameter you are wanting to search, for example num_topics or alpha / eta, make sure you substitute \"parameter_value\"\n",
    "    # into the model below instead of a static value.\n",
    "    #\n",
    "    # num topics\n",
    "    # parameter_list=[10, 25, 50, 75, 100]\n",
    "    \n",
    "    # alpha / eta\n",
    "    parameter_list=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5]\n",
    "    \n",
    "    # shuffle corpus\n",
    "    cp = list(corpus)\n",
    "    random.shuffle(cp)\n",
    "\n",
    "    # split into 80% training and 20% test sets\n",
    "    p = int(len(cp) * .8)\n",
    "    cp_train = cp[0:p]\n",
    "    cp_test = cp[p:]\n",
    "\n",
    "    # for num_topics_value in num_topics_list:\n",
    "    for parameter_value in parameter_list:\n",
    "\n",
    "        # print \"starting pass for num_topic = %d\" % num_topics_value\n",
    "        print \"starting pass for parameter_value = %.3f\" % parameter_value\n",
    "        start_time = time.time()\n",
    "\n",
    "        # run model\n",
    "        model = models.ldamodel.LdaModel(corpus=cp_train, id2word=dictionary, num_topics=40, chunksize=2000, \n",
    "                                        passes=50, update_every=0, alpha=parameter_value, eta=parameter_value, decay=0.5,\n",
    "                                        distributed=True)\n",
    "    \n",
    "        # show elapsed time for model\n",
    "        elapsed = time.time() - start_time\n",
    "        print \"Elapsed time: %s\" % elapsed\n",
    "    \n",
    "        perplex = model.bound(cp_test)\n",
    "        print \"Perplexity: %s\" % perplex\n",
    "        grid[parameter_value].append(perplex)\n",
    "    \n",
    "        per_word_perplex = np.exp2(-perplex / sum(cnt for document in cp_test for _, cnt in document))\n",
    "        print \"Per-word Perplexity: %s\" % per_word_perplex\n",
    "        grid[parameter_value].append(per_word_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 269\n",
    "# View the results of our grid search\n",
    "if(feature['perplexity_search']):\n",
    "    for parameters in grid:\n",
    "        print parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model building\n",
    "\n",
    "The LDA model was built using 75 topics. A grid search was ran across multiple topic numbers and an ideal value for number of topics was chosen which had a low perplexity.  The team members further validated the number of topics by looking at the results of various models built from various numbers of topics and choosing the one that looked best.  Grid searches were also conducted for hyper parameters `alpha` and `eta`, once again measuring perplexity.  It was found that the default parameters were reasonable.  Default `alpha` and `eta` are `1/num_topics`.\n",
    "\n",
    "## Gensim’s distributed architecture\n",
    "\n",
    "Gensim uses the [Pyro4](http://pythonhosted.org/Pyro4/) framework to allow for distributed parallel processing of LDA jobs.  The corpus is divided into chunks, and each worker is assigned a chunk.   Since we were working with 32 cores, we used 32 workers over which the entire corpus was divided.  Gensim handled all of the sharding and merging of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# CELL 270\n",
    "# Choose a model\n",
    "\n",
    "# took 24m 57s to do LDA num_topics=40, chunksize=500, passes=100, update_every=0, distributed=True \n",
    "# on my 4 core 2.66Ghz mac pro, on 37349 reviews using 8 virtual cores (8 workers)\n",
    "\n",
    "# If you don't have gensim setup for distributed set distributed=False\n",
    "# update_every is set to 0 as we are running in \"batch\" mode vs. online\n",
    "# chunksize is ideally = num documents / num workers     (when running in batch mode)\n",
    "# alpha and eta defaul to 1/num_topics if not set (None)\n",
    "if(not feature['model_load']):\n",
    "    model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics, chunksize=100, \n",
    "                                  passes=1, update_every=0, alpha=None, eta=None, decay=0.5,\n",
    "                                  distributed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 289\n",
    "# Load model\n",
    "\n",
    "if(feature['model_load']):\n",
    "    model = models.ldamodel.LdaModel.load(data_dir + '/' + file_root + '-model-t' + str(num_topics) + '.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 290\n",
    "# Save model\n",
    "\n",
    "if(feature['model_save']):\n",
    "    model.save(data_dir + '/' + file_root + '-model-t' + str(num_topics) + '.lda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 292\n",
    "# Build corpus model\n",
    "\n",
    "# create a double wrapper over the original corpus: bow->fold-in-lda, bow->tfidf->fold-in-lda, etc.\n",
    "# this is so you can view how documents are classified by the model\n",
    "if(not feature['corpus_model_load']):\n",
    "    corpus_model = model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 293\n",
    "# Load corpus model\n",
    "\n",
    "if(feature['corpus_model_load']):\n",
    "    with open(data_dir + '/' + file_root + '-corpus-model-t' + str(num_topics) + '.pickle') as f:\n",
    "        corpus_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 295\n",
    "# View corpus model\n",
    "\n",
    "# you can leave this commented unless you wish to view the contents of corpus_model in its entirity\n",
    "# I don't recommend it, because the transforms are done on the fly and it takes about 15min\n",
    "# Here you can see each review, and how it relates to each of the n topics.  Each list is a review (document) and each item \n",
    "# in the list is a tuple of (topic number,score)\n",
    "# the actual transformations, for example bow->lda are actually executed here, on the fly\n",
    "# for doc in corpus_model: \n",
    "#    print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 296\n",
    "# Save corpus model\n",
    "\n",
    "if(feature['corpus_model_save']):\n",
    "    with open(data_dir + '/' + file_root + '-corpus-model-t' + str(num_topics) + '.pickle', 'w') as f:\n",
    "        pickle.dump(corpus_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 300\n",
    "# load topics\n",
    "if(feature['model_topics_load']):\n",
    "    with open(data_dir + '/' + file_root + '-topics-t' + str(num_topics) + '.pickle') as f:\n",
    "        model_topics = pickle.load(f)\n",
    "    for num,topic in enumerate(model_topics[0]):\n",
    "        print num,topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 301\n",
    "# print topics\n",
    "model_topics = model.print_topics(num_topics)\n",
    "model_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 302\n",
    "# save topics\n",
    "if(feature['model_topics_save']):\n",
    "    with open(data_dir + '/' + file_root + '-topics-t' + str(num_topics) + '.pickle', 'w') as f:\n",
    "        pickle.dump(model_topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 310\n",
    "# These functions copied from https://groups.google.com/forum/#!searchin/gensim/visualize/gensim/SxFKsSsBTRs/cN6p3XaH4rUJ\n",
    "# They extract the gamma, beta and log probabilities \n",
    "\n",
    "def get_gamma(lda, corpus):\n",
    "    \"\"\"\n",
    "    Return gamma from a gensim LdaModel instance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lda : LdaModel\n",
    "        A fitted model.\n",
    "    corpus : gensim Corpus\n",
    "        An iterable Bag-of-Words Corpus used to fit the LDA model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gamma : ndarray\n",
    "        An ndarray that contains gamma.\n",
    "    \"\"\"\n",
    "    # lda.VAR_MAXITER = 'est' \n",
    "    chunksize = lda.chunksize\n",
    "    chunker = itertools.groupby(enumerate(corpus),\n",
    "               key=lambda (docno, doc): docno/chunksize)\n",
    "    all_gamma = []\n",
    "    for chunk_no, (key, group) in enumerate(chunker):\n",
    "        chunk = np.asarray([np.asarray(doc) for _, doc in group])\n",
    "        (gamma, sstats) = lda.inference(chunk)\n",
    "        all_gamma.append(gamma)\n",
    "    return np.vstack(all_gamma)\n",
    "\n",
    "def blei_gamma(fname, gamma):\n",
    "    \"\"\"\n",
    "    Writes the gamma file in Blei's format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Path to file to save to\n",
    "    gamma : ndarray\n",
    "        Numpy array returned from get_gamma\n",
    "    \"\"\"\n",
    "    np.savetxt(fname, gamma, fmt='%5.10f')\n",
    "\n",
    "def blei_beta(fname, lda):\n",
    "    \"\"\"\n",
    "    Write log probabilities to a space-delimited file as lda-c final.beta\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Filename\n",
    "    lda : LdaModel\n",
    "        LdaModel instance\n",
    "    \"\"\"\n",
    "    expElogbeta = np.log(lda.expElogbeta)\n",
    "    np.savetxt(fname, expElogbeta, fmt='%5.10f')\n",
    "    \n",
    "if(feature['beta_gamma_save']):\n",
    "    blei_beta(data_dir + '/' + file_root + '-t' + str(num_topics) + '.beta', model)\n",
    "    gamma = get_gamma(model, corpus)\n",
    "    blei_gamma(data_dir + '/' + file_root + '-t' + str(num_topics) + '.gamma', gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1000\n",
    "# Exploratory things you can try\n",
    "\n",
    "# set a review number to examine\n",
    "review_num=0\n",
    "\n",
    "def get_topic_data(review_num):\n",
    "    \"\"\"\n",
    "    Produces information about topics associated to a particular review\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_num : int\n",
    "        a valid review number \n",
    "    \"\"\"\n",
    "    \n",
    "    # Look at the topics associated to this review in the corpus_model\n",
    "    print \"\\nTopics for review number %d\" % review_num\n",
    "    topics = model[corpus[review_num]]\n",
    "    print topics\n",
    "    print \"\\nTopic details for topics in review number %d\" % review_num\n",
    "    topic_info = [model.print_topic(topic[0],topn=10) for topic in topics]\n",
    "    for topic in topic_info:\n",
    "        print topic\n",
    "\n",
    "# Look at this review in the corpus (bag of words, tf-idf, etc)\n",
    "# print corpus[review_num]\n",
    "\n",
    "# get data about the document as it moved through the pipeline\n",
    "if(not feature['texts_final_load']):\n",
    "    get_review_data(review_num)\n",
    "get_topic_data(review_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating topic information for each review to be written back to the database \n",
    "\n",
    "The final result we needed was a list of all topics and their percentage associated with each review.  We built a CSV file of this information for easy import back into the database.  The file contained the schools GSID, NCES Code, Universal ID and then was followed by a list of n fields, one field per topic number.  So when building a model with 75 topics, as our final model was, we had 75 topic value fields.  These would correspond with a percentage the review was represented by that topic.  A review could have multiple topics it corresponded to.  Any topic correlation that was less than 1% was just kept at 0.  Later these values would all be normalized before being stored in the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# CELL 1100 \n",
    "# Create CSV file to place topic numbers into the database for each review\n",
    "\n",
    "with open(data_dir + '/' + file_root + '-review_topics-t' + str(num_topics) + '.csv', 'wb') as csvfile:\n",
    "    topicwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for row in xrange(len(reviews)):\n",
    "        # we store gsid, nces_code and universal_id in topic_array_1 which is str (some nces_codes are alphanum)\n",
    "        topic_array_1 = np.empty(3, dtype=\"S10\")\n",
    "        # we store each topic percentage in topic_array_2 which is floats\n",
    "        topic_array_2 = np.zeros(num_topics, float)\n",
    "        \n",
    "        if(row % 1000 == 0):\n",
    "            print \"Processing row %d\" % row\n",
    "        gsid, nces_code, universal_id, postdate = reviews_indexes.irow(row)\n",
    "        # some gsid's are None because the school had no reviews\n",
    "        if(gsid is None):\n",
    "            gsid = 999999999\n",
    "        topic_array_1[0], topic_array_1[1], topic_array_1[2] = gsid, nces_code, universal_id\n",
    "\n",
    "        # we get the topics from the model\n",
    "        topics = model[corpus[row]]\n",
    "    \n",
    "        # we update the topic_array_2 with the percentage values for each topic\n",
    "        for topic in topics:\n",
    "            topic_num,topic_score = topic\n",
    "            topic_array_2[topic_num] = topic_score\n",
    "         \n",
    "        # we combine both arrays into a single writerow\n",
    "        topicwriter.writerow(list(topic_array_1) + list(topic_array_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Cells for Word Frequency Counts\n",
    "The below cells are just used in a very ad-hoc way to produce word counts of different slices and cuts of the data, and export them into CSV format.  We use a similar design paradigm as our original work earlier in the notebook, but its different enough that we could not re-use it as is.  If we had more time we could factor together much of the code/functionality below into some more generalized functions.  The below cells are not even controlled feature the feature dict, they are just ad-hoc cells to be ran manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1500 \n",
    "# Word Frequency Counts\n",
    "# remove encodings, tokenize, remove punctuation, remove stopwords, remove words < 3, remove non alpha words\n",
    "\n",
    "# This assumes paralell architecture has been setup in CELLS 75 and 85 and those CELLS have been ran, otherwise this will fail.\n",
    "# The below function freq_clean() is not designed to be ran serial.\n",
    "\n",
    "@dv.remote(block=True)\n",
    "def freq_clean():\n",
    "    \"\"\"\n",
    "    tokenizes and cleans the text that exists in data_to_clean\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    reviews : list of lists\n",
    "        a tokenized text\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # we need to strip escape characters so we compile a pattern\n",
    "    hexchars = re.compile('\\\\\\\\x(\\w{2})')\n",
    "        \n",
    "    print \"Starting to process %d documents\" % len(data_to_clean)\n",
    "\n",
    "    for review_text in data_to_clean:\n",
    "        \n",
    "        # print result to stdout, note that when using parallel stdout is not sent to the Client\n",
    "        # counter += 1\n",
    "        # if ((counter % 1000)==0):\n",
    "        #    print \"%d documents processed\" % counter\n",
    "        # sys.stdout.flush()\n",
    "                          \n",
    "        # remove html encodings\n",
    "        review_text = review_text.replace('&amp;','').replace('&lt;','').replace('&gt;','').replace('&quot;','').replace('&#039;','').replace('&#034;','')\n",
    "        review_text = re.sub(hexchars, '', review_text.encode(\"string-escape\"))\n",
    "      \n",
    "        # tokenize, you don't want to turn this off\n",
    "        review_text = [word for sent in sent_tokenize(review_text) for word in word_tokenize(sent)]\n",
    "\n",
    "        # remove punctuation\n",
    "        review_text = [word.translate(string.maketrans(\"\",\"\"), string.punctuation) for word in review_text]\n",
    "                    \n",
    "        # remove smallwords\n",
    "        review_text = [word for word in review_text if not len(word) < 3]\n",
    "        \n",
    "        # lowercase all text\n",
    "        review_text = [word.lower() for word in review_text]\n",
    "        \n",
    "        # remove stopwords\n",
    "        review_text = [word for word in review_text if not word in stopset_to_use]\n",
    "        \n",
    "        texts.append(review_text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1510\n",
    "# Merge Freq Count data back to one variable\n",
    "\n",
    "def freq_merge(result):\n",
    "    \"\"\"\n",
    "    Merges parallelized data back into a single structure\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    result : array\n",
    "        An array containing the values returned from each thread/worker\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    texts : list of lists\n",
    "        a tokenized text\n",
    "    \"\"\"\n",
    "    # Merge Parallel Data back from remote workers back into one set\n",
    "    texts = []\n",
    "    \n",
    "    # iterate through each worker and merge the texts back together\n",
    "    for worker in result:\n",
    "        texts += worker\n",
    "    print \"Total after merge: %d\" % len(texts)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1520\n",
    "# get reviews from database\n",
    "\n",
    "# get reviews for a single state\n",
    "def get_state(state):\n",
    "    \"\"\"\n",
    "    Gets all reviews from the database for a single state\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state : string\n",
    "        abbreviation of state\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    reviews : list of lists\n",
    "        a tokenized text\n",
    "    \"\"\"\n",
    "    cnx = mysql_ops()\n",
    "    reviews = cnx.get_reviews_state(state)[['reviews']]\n",
    "    print \"Ingested %d documents from database for state %s\" % (len(reviews),state)\n",
    "    return reviews\n",
    "\n",
    "# get all reviews\n",
    "def get_all():\n",
    "    \"\"\"\n",
    "    Gets all reviews from the database\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    reviews : list of lists\n",
    "        a tokenized text\n",
    "    \"\"\"\n",
    "    cnx = mysql_ops()\n",
    "    reviews = cnx.get_all_reviews()\n",
    "    print \"Ingested %d documents from database for all states\" % len(reviews)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1530\n",
    "# write freq counts to a file\n",
    "\n",
    "def freq_write(texts,state):\n",
    "    \"\"\"\n",
    "    Write word frequency count to file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of lists\n",
    "        a tokenized text\n",
    "    state : string\n",
    "        name of state or other slice name\n",
    "    \"\"\"\n",
    "    f = open(data_dir + '/' + state + '-freq.txt','w')\n",
    "    print \"Writing file for state %s\" % state\n",
    "    fdist = nltk.FreqDist(list(itertools.chain.from_iterable(texts)))\n",
    "    for word,count in fdist.items()[:25]:\n",
    "        f.write(\"%s,%s\\n\" % (word,count))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1540\n",
    "# get all reviews\n",
    "\n",
    "reviews_all = get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1550\n",
    "# slice and cut the reviews in various ways we wish to do our counts\n",
    "\n",
    "# stars_0 = reviews_all[reviews_all['stars'] == 99]\n",
    "# stars_1 = reviews_all[reviews_all['stars'] == 1]\n",
    "# stars_2 = reviews_all[reviews_all['stars'] == 2]\n",
    "# stars_3 = reviews_all[reviews_all['stars'] == 3]\n",
    "# stars_4 = reviews_all[reviews_all['stars'] == 4]\n",
    "# stars_5 = reviews_all[reviews_all['stars'] == 5]\n",
    "# reviewer_parent = reviews_all[reviews_all['reviewer'] == 'parent']\n",
    "# reviewer_student = reviews_all[reviews_all['reviewer'] == 'student']\n",
    "# reviewer_other = reviews_all[reviews_all['reviewer'] == 'other']\n",
    "# reviewer_teacher = reviews_all[reviews_all['reviewer'] == 'teacher']\n",
    "# reviewer_former_student = reviews_all[reviews_all['reviewer'] == 'former student']\n",
    "# reviewer_empty = reviews_all[reviews_all['reviewer'] == '']\n",
    "# reviewer_staff = reviews_all[reviews_all['reviewer'] == 'staff']\n",
    "# reviewer_administrator = reviews_all[reviews_all['reviewer'] == 'administrator']\n",
    "# reviewer_principal = reviews_all[reviews_all['reviewer'] == 'principal']\n",
    "# type_public = reviews_all[reviews_all['type'] == 'public']\n",
    "# type_private = reviews_all[reviews_all['type'] == 'private']\n",
    "# type_charter = reviews_all[reviews_all['type'] == 'charter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1560\n",
    "# examine lengths\n",
    "\n",
    "# print len(stars_0)\n",
    "# print len(stars_1)\n",
    "# print len(stars_2)\n",
    "# print len(stars_3)\n",
    "# print len(stars_4)\n",
    "# print len(stars_5)\n",
    "# print len(reviewer_parent)\n",
    "# print len(reviewer_student)\n",
    "# print len(reviewer_other)\n",
    "# print len(reviewer_teacher)\n",
    "# print len(reviewer_former_student)\n",
    "# print len(reviewer_empty)\n",
    "# print len(reviewer_staff)\n",
    "# print len(reviewer_administrator)\n",
    "# print len(reviewer_principal)\n",
    "# print len(type_public)\n",
    "# print len(type_private)\n",
    "# print len(type_charter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 1570\n",
    "# examine the value counts of any type we wish just to make sure we sliced all the values\n",
    "\n",
    "print reviews_all['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# CELL 1580\n",
    "# main pipeline to process freq counts\n",
    "\n",
    "states = ['AK','AL','AR','AZ','CA','CO','CT','DC','DE','FL',\n",
    "          'GA','HI','IA','ID','IL','IN','KS','KY','LA','MA',\n",
    "          'MD','ME','MI','MN','MO','MS','MT','NC','ND','NE',\n",
    "          'NH','NJ','NM','NV','NY','OH','OK','OR','PA','PR',\n",
    "          'RI','SC','SD','TN','TX','UT','VA','VT','WA','WI',\n",
    "          'WV','WY']\n",
    "\n",
    "# For processing states you can use it as is.  For processing other types of slices, you would comment out the\n",
    "# for loop and unindent its code, and then change the reviews= line and state= to match the slice your working with.\n",
    "# It's very crude as this was done toward the end, but it worked nicely.\n",
    "for state in states:\n",
    "    # state = 'type_charter'\n",
    "    reviews = get_state(state)\n",
    "    \n",
    "    print \"Processing state %s\" % state\n",
    "\n",
    "    reviews = type_charter['reviews']\n",
    "    \n",
    "    data_to_clean = reviews\n",
    "    stopset_to_use = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # copy the data_to_clean and stopset_to_use into name space of remote workers \n",
    "    # We use ipythons parallel processing \"scatter\" to shard \"reviews\" accross our cores\n",
    "    dv.scatter('data_to_clean',data_to_clean)\n",
    "    \n",
    "    # We copy the objects feature and stopset, as well as the Class SpellingReplacer to our remote workers\n",
    "    dv['stopset_to_use'] = stopset_to_use\n",
    "    \n",
    "    # Show size of entire data being sent for processing\n",
    "    print \"Total data size: %d\" % len(data_to_clean)\n",
    "    \n",
    "    # Show the shard size of each remote worker\n",
    "    print \"Sharded data size for each worker\"\n",
    "    %px print len(data_to_clean)\n",
    "    \n",
    "    # clean the reviews\n",
    "    result = freq_clean()\n",
    "    \n",
    "    # merge results back\n",
    "    texts = freq_merge(result)\n",
    "    \n",
    "    # write file \n",
    "    freq_write(texts,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We selected the model based on out-of-sample perplexity.  We used code from the <a href=\"http://nlp.stanford.edu/downloads/tmt/tmt-0.4/\">Stanford topic modeling toolkit</a> to calculate out of sample perplexity.  Samples of 50,000 and 80,000 reviews were used, and run iteration of 500 and 1,500 were tested for differences.\n",
    "\n",
    "Surprisingly, the number of reviews did not matter very much.  The perplexity is similar between 50,000 and 80,000 reviews.  The elbow appears around 50-80.  The number of iterations also did not seem to change after a large number of topics.\n",
    "\n",
    "<img src=\"files/img_bf-notebook-perplex.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic naming\n",
    "\n",
    "To name the topics, we reviewed the top 10 most frequent words associated with each topic, as well as the top 10 most highly scored review associated with that topic, i.e., the reviews most likely to fall into that topic.  Using blind review by two team members, then verification by all team members, we classified the 75 topics into 30 8 categories and 30 subcategories.  A minimum of 13 hours was spent completing this task to ensure that the names most accurately captured the topic estimated by LDA.\n",
    "\n",
    "Future work in this area could include:\n",
    "\n",
    "- Use <a href=\"https://www.mturk.com/mturk/\">Amazon Mechanical Turk</a> to verify naming\n",
    "\n",
    "- Use tree-related text analysis algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
